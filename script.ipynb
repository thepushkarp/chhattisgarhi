{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1193d97-f487-43ea-abbc-88b9640c390e",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6adb6d7b-71ce-4563-ac09-b27060f737a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import validators\n",
    "from urllib.parse import urlparse\n",
    "import json\n",
    "import string\n",
    "from string import digits\n",
    "from collections import Counter\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec89c318-e3a9-4f7a-9902-6ca992412268",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaee84a8-b38e-4b46-a941-ba3b02390d4e",
   "metadata": {},
   "source": [
    "# Initializing the Google Sheets API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0de3da55-a05d-46f9-9431-68aa6ad40e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ID and range of a sample spreadsheet.\n",
    "SPREADSHEET_ID = os.getenv(\"SPREADSHEET_ID\")\n",
    "GENERAL_RANGE = \"GENERAL!C:S\"\n",
    "AGRI_RANGE = \"AGRICULTURE!C:S\"\n",
    "FIN_RANGE = \"FINANCE!C:S\"\n",
    "DUP_RANGE = \"DUPLICATES!A1:A1005\"\n",
    "UN_RANGE = \"UNADDED!A:B\"\n",
    "EX_RANGE = \"EXTRA!A1:A10001\"\n",
    "RANGES = [AGRI_RANGE, FIN_RANGE, GENERAL_RANGE]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810ec98f-54b5-4551-b3f8-4ae6545d33fc",
   "metadata": {},
   "source": [
    "# Accessing Sheets, finding duplicates and unadded links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3247141-ecae-4dd2-b6c3-59e89402f348",
   "metadata": {},
   "source": [
    "## Generating token to access the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0a63e51-eb56-49a2-aa73-17b0b83dce0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If modifying these scopes, delete the file write-token.json and read-token.json\n",
    "\n",
    "# SCOPES = [\"https://www.googleapis.com/auth/spreadsheets.readonly\"] # For read-only scope\n",
    "SCOPES = [\"https://www.googleapis.com/auth/spreadsheets\"]  # For writing to sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c6000c7-b728-4156-88ce-78262136790a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=866438115849-109dmktdu3k3c8su4ict16qf3m8ehqal.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A60549%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fspreadsheets&state=FkuxdTFIkt6kJmqjoDx6lGYvrNwU76&access_type=offline\n"
     ]
    }
   ],
   "source": [
    "creds = None\n",
    "\n",
    "if os.path.exists(\"write-token.json\"):\n",
    "    creds = Credentials.from_authorized_user_file(\"write-token.json\", SCOPES)\n",
    "# If there are no (valid) credentials available, let the user log in.\n",
    "if not creds or not creds.valid:\n",
    "    if creds and creds.expired and creds.refresh_token:\n",
    "        creds.refresh(Request())\n",
    "    else:\n",
    "        flow = InstalledAppFlow.from_client_secrets_file(\"credentials.json\", SCOPES)\n",
    "        creds = flow.run_local_server(port=0)\n",
    "    # Save the credentials for the next run\n",
    "    with open(\"write-token.json\", \"w\") as token:\n",
    "        token.write(creds.to_json())\n",
    "\n",
    "service = build(\"sheets\", \"v4\", credentials=creds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0971fae-2ab5-48ab-af3d-53a5b728d08b",
   "metadata": {},
   "source": [
    "## Getting Sitemaps from Gurturgoth and Anjor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33718d98-478c-4cb7-beb7-6fbe881e0079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://hanka.gurturgoth.com/post-sitemap1.xml',\n",
       " 'https://hanka.gurturgoth.com/post-sitemap2.xml',\n",
       " 'https://hanka.gurturgoth.com/post-sitemap3.xml',\n",
       " 'https://hanka.gurturgoth.com/post-sitemap4.xml',\n",
       " 'https://hanka.gurturgoth.com/post-sitemap5.xml',\n",
       " 'https://hanka.gurturgoth.com/post-sitemap6.xml']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gurturgoth Sitemaps\n",
    "gg_sitemaps = [\n",
    "    f\"https://hanka.gurturgoth.com/post-sitemap{i}.xml\" for i in range(1, 6 + 1)\n",
    "]\n",
    "gg_sitemaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3501581c-c7d8-44b6-97d8-f1a4d628cc82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.anjor.online/sitemap.xml?page=1',\n",
       " 'https://www.anjor.online/sitemap.xml?page=2',\n",
       " 'https://www.anjor.online/sitemap.xml?page=3',\n",
       " 'https://www.anjor.online/sitemap.xml?page=4',\n",
       " 'https://www.anjor.online/sitemap.xml?page=5',\n",
       " 'https://www.anjor.online/sitemap.xml?page=6',\n",
       " 'https://www.anjor.online/sitemap.xml?page=7',\n",
       " 'https://www.anjor.online/sitemap.xml?page=8',\n",
       " 'https://www.anjor.online/sitemap.xml?page=9',\n",
       " 'https://www.anjor.online/sitemap.xml?page=10',\n",
       " 'https://www.anjor.online/sitemap.xml?page=11',\n",
       " 'https://www.anjor.online/sitemap.xml?page=12',\n",
       " 'https://www.anjor.online/sitemap.xml?page=13']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Anjor Sitemaps\n",
    "an_sitemaps = [\n",
    "    f\"https://www.anjor.online/sitemap.xml?page={i}\" for i in range(1, 13 + 1)\n",
    "]\n",
    "an_sitemaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "512e645a-ccd9-4cff-aed7-e1e35fb8be54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sitemap_links(sitemaps):\n",
    "    \"\"\"\n",
    "    Returns all the URLs found in the sitemaps\n",
    "    \"\"\"\n",
    "\n",
    "    all_urls = set()\n",
    "    header = {\n",
    "        \"Accept\": \"*/*\",\n",
    "        \"Accept-Language\": \"*\",\n",
    "        \"Accept-Encoding\": \"identity, gzip, deflate, compress, br\",\n",
    "        \"User-Agent\": \"XY\",\n",
    "    }\n",
    "\n",
    "    for sitemap in sitemaps:\n",
    "        r = requests.get(sitemap, headers=header)\n",
    "        xml = r.text\n",
    "\n",
    "        soup = BeautifulSoup(xml)\n",
    "        URLTags = soup.find_all(\"url\")\n",
    "\n",
    "        print(f\"{sitemap}: {len(URLTags)} urls found\")\n",
    "\n",
    "        for URL in URLTags:\n",
    "            all_urls.add(URL.findNext(\"loc\").text)\n",
    "\n",
    "    print(f\"Total: {len(all_urls)} links found\")\n",
    "\n",
    "    return list(all_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a30c3ba9-457a-4d78-8115-2ac23dd4c027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://hanka.gurturgoth.com/post-sitemap1.xml: 1000 urls found\n",
      "https://hanka.gurturgoth.com/post-sitemap2.xml: 1000 urls found\n",
      "https://hanka.gurturgoth.com/post-sitemap3.xml: 1000 urls found\n",
      "https://hanka.gurturgoth.com/post-sitemap4.xml: 1000 urls found\n",
      "https://hanka.gurturgoth.com/post-sitemap5.xml: 1000 urls found\n",
      "https://hanka.gurturgoth.com/post-sitemap6.xml: 57 urls found\n",
      "Total: 5057 links found\n"
     ]
    }
   ],
   "source": [
    "gg_links = get_sitemap_links(gg_sitemaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8a91e51-f202-4724-88f6-7eb80f99a02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.anjor.online/sitemap.xml?page=1: 150 urls found\n",
      "https://www.anjor.online/sitemap.xml?page=2: 150 urls found\n",
      "https://www.anjor.online/sitemap.xml?page=3: 150 urls found\n",
      "https://www.anjor.online/sitemap.xml?page=4: 150 urls found\n",
      "https://www.anjor.online/sitemap.xml?page=5: 150 urls found\n",
      "https://www.anjor.online/sitemap.xml?page=6: 150 urls found\n",
      "https://www.anjor.online/sitemap.xml?page=7: 150 urls found\n",
      "https://www.anjor.online/sitemap.xml?page=8: 150 urls found\n",
      "https://www.anjor.online/sitemap.xml?page=9: 150 urls found\n",
      "https://www.anjor.online/sitemap.xml?page=10: 150 urls found\n",
      "https://www.anjor.online/sitemap.xml?page=11: 150 urls found\n",
      "https://www.anjor.online/sitemap.xml?page=12: 150 urls found\n",
      "https://www.anjor.online/sitemap.xml?page=13: 71 urls found\n",
      "Total: 1871 links found\n"
     ]
    }
   ],
   "source": [
    "an_links = get_sitemap_links(an_sitemaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5d803a-d644-48c6-b3ed-a52a6bb9209b",
   "metadata": {},
   "source": [
    "## Save the sitemaps as CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f468cfe7-f7b9-4819-9d0e-aa497672d7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_csv(filename, links, foldername=\"\"):\n",
    "    \"\"\"\n",
    "    Saves the sitemaps as CSV files\n",
    "    filename: Name of the file to save the csv as\n",
    "    links: a collection of links to save in the csv\n",
    "    foldername: Name of the folder to save the file\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(data={\"links\": list(links)})\n",
    "    if foldername:\n",
    "        if not os.path.exists(foldername):\n",
    "            os.mkdir(foldername)\n",
    "        df.to_csv(os.path.join(foldername, filename), sep=\",\", index=False)\n",
    "    else:\n",
    "        df.to_csv(filename, sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5c15728-7c8c-4924-aa9d-745a895263ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_csv(\"anjor.csv\", an_links, \"sitemaps\")\n",
    "save_as_csv(\"gurtur.csv\", gg_links, \"sitemaps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124aabb7-8afc-4df0-89d9-b010024001a5",
   "metadata": {},
   "source": [
    "## Get the data summary from Google Sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2acde233-52fc-452a-aa18-4c93e85705d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 ranges retrieved.\n",
      "Total links in AGRICULTURE!C1:S1236:\t358\n",
      "Total links in FINANCE!C1:S1234:\t234\n",
      "Total links in GENERAL!C1:S1943:\t820\n",
      "\n",
      "Total links in sheet:\t1412\n",
      "Unique links in sheet:\t1412\n",
      "Duplicate links:\t0\n"
     ]
    }
   ],
   "source": [
    "# Call the Sheets API\n",
    "sheet = service.spreadsheets()\n",
    "result = sheet.values().batchGet(spreadsheetId=SPREADSHEET_ID, ranges=RANGES).execute()\n",
    "ranges = result.get(\"valueRanges\", [])\n",
    "\n",
    "sheet_links = set()\n",
    "duplicate_links = set()\n",
    "link_count = 0\n",
    "\n",
    "if not ranges:\n",
    "    print(\"No data found.\")\n",
    "else:\n",
    "    print(f\"{len(ranges)} ranges retrieved.\")\n",
    "    for single_range in ranges:\n",
    "        range_count = 0\n",
    "        for row in single_range[\"values\"]:\n",
    "            if len(row) != 0:\n",
    "                for item in row:\n",
    "                    if validators.url(item):\n",
    "                        range_count += 1\n",
    "                        if item in sheet_links:\n",
    "                            duplicate_links.add(item)\n",
    "                        else:\n",
    "                            sheet_links.add(item)\n",
    "        print(f\"Total links in {single_range['range']}:\\t{range_count}\")\n",
    "        link_count += range_count\n",
    "    print()\n",
    "    print(f\"Total links in sheet:\\t{link_count}\")\n",
    "    print(f\"Unique links in sheet:\\t{len(sheet_links)}\")\n",
    "    print(f\"Duplicate links:\\t{len(duplicate_links)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe8b9040-4754-4498-b799-f21f5fb3a3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Verify duplicates\n",
    "count = 0\n",
    "for link in list(duplicate_links):\n",
    "    if link in sheet_links:\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2bca178-3194-4fa8-9e7b-3f669b4b4fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the duplicates\n",
    "save_as_csv(\"duplicates.csv\", duplicate_links, \"duplicates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9484f9-2539-4883-8f1f-febe1b9e604a",
   "metadata": {},
   "source": [
    "## Writing duplicates to Google Sheets [Caution: Can overwrite to sheet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a57843c-699a-47e6-97af-bfe3cf9342a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing to Google Sheets\n",
    "values = [[\"DUPLICATES\"]]\n",
    "\n",
    "for value in list(duplicate_links):\n",
    "    values.append([value])\n",
    "\n",
    "len_values = len(values)\n",
    "\n",
    "for _ in range(len_values, 1000 + 1):\n",
    "    values.append([\"\"])\n",
    "\n",
    "body = {\"values\": values}\n",
    "\n",
    "value_input_option = \"USER_ENTERED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d7e5ec3-22c4-41de-b5fa-3f2b955877e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001 cells updated.\n"
     ]
    }
   ],
   "source": [
    "result = (\n",
    "    service.spreadsheets()\n",
    "    .values()\n",
    "    .update(\n",
    "        spreadsheetId=SPREADSHEET_ID,\n",
    "        range=DUP_RANGE,\n",
    "        valueInputOption=value_input_option,\n",
    "        body=body,\n",
    "    )\n",
    "    .execute()\n",
    ")\n",
    "print(f\"{result.get('updatedCells')} cells updated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2804bf1-88d1-4917-b150-22baf63c3cc9",
   "metadata": {},
   "source": [
    "## Finding links Gurturgoth and Anjor that are not in the sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78632c45-8ad3-4817-b891-94f47e26c045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 rows retrieved.\n"
     ]
    }
   ],
   "source": [
    "# Getting the un-needed extra links\n",
    "result = (\n",
    "    service.spreadsheets()\n",
    "    .values()\n",
    "    .get(spreadsheetId=SPREADSHEET_ID, range=EX_RANGE)\n",
    "    .execute()\n",
    ")\n",
    "extra = result.get(\"values\", [])\n",
    "print(f\"{len(extra)} rows retrieved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5060c632-660d-48eb-9855-c95e9970abd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Gurturgoth links not in sheet: 3850\n"
     ]
    }
   ],
   "source": [
    "# Gurturgoth links not in sheet\n",
    "unadded_gg_links = []\n",
    "for link in gg_links:\n",
    "    if link not in sheet_links and link not in extra:\n",
    "        unadded_gg_links.append(link)\n",
    "\n",
    "print(f\"Number of Gurturgoth links not in sheet: {len(unadded_gg_links)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0aa51cbe-fbba-497e-bbfe-ae1e2c50f3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Anjor links not in sheet: 1712\n"
     ]
    }
   ],
   "source": [
    "# Anjor links not in sheet\n",
    "unadded_an_links = []\n",
    "for link in an_links:\n",
    "    if link not in sheet_links and link not in extra:\n",
    "        unadded_an_links.append(link)\n",
    "\n",
    "print(f\"Number of Anjor links not in sheet: {len(unadded_an_links)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d10de2d4-b591-4cd9-99c9-49d93a75e605",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_csv(\"unadded_gg_links.csv\", unadded_gg_links, \"unadded_links\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "939308cf-a6f0-4125-af8b-4b4d8d6331d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_csv(\"unadded_an_links.csv\", unadded_an_links, \"unadded_links\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b2e186-54d1-4c29-8b67-b43f3cc117e6",
   "metadata": {},
   "source": [
    "## Writing unadded links to Sheet [Caution: Can overwrite to sheet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07ee2830-1d7a-45fd-9982-020280ad696f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing to Google Sheets\n",
    "values = [[\"GURTUR\", \"ANJOR\"]]\n",
    "\n",
    "for i, gg_value in enumerate(list(unadded_gg_links)):\n",
    "    an_value = \"\"\n",
    "    if i < len(unadded_an_links):\n",
    "        an_value = unadded_an_links[i]\n",
    "    values.append([gg_value, an_value])\n",
    "\n",
    "len_values = len(values)\n",
    "\n",
    "for _ in range(len_values, 5000 + 1):\n",
    "    values.append([\"\", \"\"])\n",
    "\n",
    "body = {\"values\": values}\n",
    "\n",
    "value_input_option = \"USER_ENTERED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e562e080-86f0-4dbc-acc5-62278518f980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10002 cells updated.\n"
     ]
    }
   ],
   "source": [
    "result = (\n",
    "    service.spreadsheets()\n",
    "    .values()\n",
    "    .update(\n",
    "        spreadsheetId=SPREADSHEET_ID,\n",
    "        range=UN_RANGE,\n",
    "        valueInputOption=value_input_option,\n",
    "        body=body,\n",
    "    )\n",
    "    .execute()\n",
    ")\n",
    "print(f\"{result.get('updatedCells')} cells updated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fab91d-3c10-46aa-a6dc-324bbc3be18a",
   "metadata": {},
   "source": [
    "# Extracting data from the collected links and cleaning them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938b1207-c356-44ac-82bd-2cb5493e9f50",
   "metadata": {},
   "source": [
    "## Generating token to access the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "953c9440-5fd4-4f8a-9e7a-3b44364f6cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If modifying these scopes, delete the file write-token.json and read-token.json\n",
    "\n",
    "SCOPES = [\n",
    "    \"https://www.googleapis.com/auth/spreadsheets.readonly\"\n",
    "]  # For read-only scope\n",
    "# SCOPES = [\"https://www.googleapis.com/auth/spreadsheets\"]  # For writing to sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "186dd8d6-e1d8-4ca5-89e9-e53225aa5876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=866438115849-109dmktdu3k3c8su4ict16qf3m8ehqal.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A61210%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fspreadsheets.readonly&state=JAVOQTGIidz7BdlB79JKCM72xtwLNp&access_type=offline\n"
     ]
    }
   ],
   "source": [
    "creds = None\n",
    "\n",
    "if os.path.exists(\"read-token.json\"):\n",
    "    creds = Credentials.from_authorized_user_file(\"read-token.json\", SCOPES)\n",
    "# If there are no (valid) credentials available, let the user log in.\n",
    "if not creds or not creds.valid:\n",
    "    if creds and creds.expired and creds.refresh_token:\n",
    "        creds.refresh(Request())\n",
    "    else:\n",
    "        flow = InstalledAppFlow.from_client_secrets_file(\"credentials.json\", SCOPES)\n",
    "        creds = flow.run_local_server(port=0)\n",
    "    # Save the credentials for the next run\n",
    "    with open(\"read-token.json\", \"w\") as token:\n",
    "        token.write(creds.to_json())\n",
    "\n",
    "service = build(\"sheets\", \"v4\", credentials=creds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d8be87-2eb1-4e4e-b53d-c3fb5b4bcf77",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a72a60dd-e559-467a-9b79-86ca65b5dd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_gurtur(link):\n",
    "    parsed_uri = urlparse(link)\n",
    "    result = \"{uri.scheme}://{uri.netloc}/\".format(uri=parsed_uri)\n",
    "    return result == \"https://hanka.gurturgoth.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97017eed-4077-4141-9e2f-625c3a0f199c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_anjor(link):\n",
    "    parsed_uri = urlparse(link)\n",
    "    result = \"{uri.scheme}://{uri.netloc}/\".format(uri=parsed_uri)\n",
    "    return result == \"https://www.anjor.online/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "742f7c07-68aa-4e7d-a720-fd76392c4fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_patrika(link):\n",
    "    parsed_uri = urlparse(link)\n",
    "    result = \"{uri.scheme}://{uri.netloc}/\".format(uri=parsed_uri)\n",
    "    return result == \"https://www.patrika.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e4cde293-19d2-4432-8b31-84ae7270118f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "assert is_gurtur(\"https://hanka.gurturgoth.com/mrs-devendra-kumari-singhdev/\") == True\n",
    "assert is_gurtur(\"https://www.anjor.online/2021/05/baldi-bai.jpg.html\") == False\n",
    "assert is_anjor(\"https://www.anjor.online/2021/05/baldi-bai.jpg.html\") == True\n",
    "assert is_anjor(\"https://hanka.gurturgoth.com/mrs-devendra-kumari-singhdev/\") == False\n",
    "assert is_patrika(\"https://hanka.gurturgoth.com/mrs-devendra-kumari-singhdev/\") == False\n",
    "assert is_patrika(\"https://www.anjor.online/2021/05/baldi-bai.jpg.html\") == False\n",
    "assert (\n",
    "    is_patrika(\"https://www.patrika.com/raipur-news/chattisgarhi-sahitya-6522552/\")\n",
    "    == True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ea0f95bc-0503-48d4-8f83-eedc566f4f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_chars = 128\n",
    "devnagri_chars = []\n",
    "for i in range(num_chars):\n",
    "    devnagri_chars.append(chr(ord(\"ऀ\") + i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "16cb755d-af0a-4d32-bf10-b13d9f030162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_devnagri(text):\n",
    "    return any(dev_char in text for dev_char in devnagri_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0ae678-a8f8-44e0-a696-416d564ae276",
   "metadata": {},
   "source": [
    "## Get data from Sheets API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1802952f-1fb0-4f05-945c-c176aae5fb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_link(link):\n",
    "    \"\"\"\n",
    "    Gets the data from page source and extracts the contents if the\n",
    "    page belongs to one of the three sites (Gurtur,Anjor, Patrika)\n",
    "    \"\"\"\n",
    "    header = {\n",
    "        \"Accept\": \"*/*\",\n",
    "        \"Accept-Language\": \"*\",\n",
    "        \"Accept-Encoding\": \"identity, gzip, deflate, compress, br\",\n",
    "        \"User-Agent\": \"XY\",\n",
    "    }\n",
    "    content = \"\"\n",
    "\n",
    "    try:\n",
    "        page = requests.get(link, headers=header)\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    except:\n",
    "        return content\n",
    "\n",
    "    if is_gurtur(link):\n",
    "        if content_soup := soup.find(\"div\", class_=\"entry-content\"):\n",
    "            contents = []\n",
    "            if tags := content_soup.find_all([\"p\", \"span\", \"div\"]):\n",
    "                for tag in tags:\n",
    "                    useful_text = \"\"\n",
    "                    for text in tag.find_all(text=True):\n",
    "                        if has_devnagri(text):\n",
    "                            useful_text = \" \".join([useful_text, text.strip()])\n",
    "                    contents.append(useful_text)\n",
    "            content = (\" \").join(contents).strip()\n",
    "\n",
    "    elif is_anjor(link):\n",
    "        if content_soup := soup.find(\"div\", class_=\"entry-content\"):\n",
    "            contents = []\n",
    "            if tags := content_soup.find_all([\"p\", \"span\", \"div\"]):\n",
    "                for tag in tags:\n",
    "                    useful_text = \"\"\n",
    "                    for text in tag.find_all(text=True):\n",
    "                        if has_devnagri(text):\n",
    "                            useful_text = \" \".join([useful_text, text.strip()])\n",
    "                    contents.append(useful_text)\n",
    "            content = (\" \").join(contents).strip()\n",
    "\n",
    "    elif is_patrika(link):\n",
    "        if content_soup := soup.find(\"div\", class_=\"complete-story\"):\n",
    "            contents = []\n",
    "            if tags := content_soup.find_all([\"p\", \"span\", \"div\"]):\n",
    "                for tag in tags:\n",
    "                    useful_text = \"\"\n",
    "                    for text in tag.find_all(text=True):\n",
    "                        if has_devnagri(text):\n",
    "                            useful_text = \" \".join([useful_text, text.strip()])\n",
    "                    contents.append(useful_text)\n",
    "            content = (\" \").join(contents).strip()\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "53ab7ff9-461b-4a05-9627-34c23258ee7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(ranges_list=[\"AGRI\", \"FIN\", \"GEN\"], verbose=False):\n",
    "    \"\"\"\n",
    "    Extracts the links from the given domain and stores them in json files\n",
    "    \"\"\"\n",
    "\n",
    "    ranges_to_extract = []\n",
    "    if \"AGRI\" in ranges_list:\n",
    "        ranges_to_extract.append(AGRI_RANGE)\n",
    "    if \"FIN\" in ranges_list:\n",
    "        ranges_to_extract.append(FIN_RANGE)\n",
    "    if \"GEN\" in ranges_list:\n",
    "        ranges_to_extract.append(GENERAL_RANGE)\n",
    "\n",
    "    # Call the Sheets API\n",
    "    sheet = service.spreadsheets()\n",
    "    result = (\n",
    "        sheet.values()\n",
    "        .batchGet(spreadsheetId=SPREADSHEET_ID, ranges=ranges_to_extract)\n",
    "        .execute()\n",
    "    )\n",
    "    ranges = result.get(\"valueRanges\", [])\n",
    "\n",
    "    unscraped = []\n",
    "\n",
    "    if not os.path.exists(\"check\"):\n",
    "        os.mkdir(\"check\")\n",
    "    check_f = open(os.path.join(\"check\", \"check_content.txt\"), \"w\", encoding=\"utf-8\")\n",
    "\n",
    "    sheet_links = set()\n",
    "    link_count = 0\n",
    "    last_topic = \"\"\n",
    "\n",
    "    if not ranges:\n",
    "        print(\"No data found.\")\n",
    "    else:\n",
    "        print(f\"{len(ranges)} ranges retrieved\\n\")\n",
    "\n",
    "        # Iterate over all domains in ranges\n",
    "        for range_index, single_range in enumerate(ranges):\n",
    "            print(f\"In range {ranges_list[range_index]}\\n\")\n",
    "            range_count = 0\n",
    "\n",
    "            # Create folder if not already exists\n",
    "            folder_name = single_range[\"range\"].split(\"!\")[0]\n",
    "            if not os.path.exists(folder_name):\n",
    "                os.makedirs(folder_name)\n",
    "\n",
    "            for i, row in enumerate(single_range[\"values\"]):\n",
    "                index = 0\n",
    "                if len(row) != 0 and i != 0:\n",
    "\n",
    "                    # If topic exists in row\n",
    "                    if row[0]:\n",
    "                        last_topic = row[0]\n",
    "                        subtopic = \"\"\n",
    "                        if verbose:\n",
    "                            print(f\"Inside topic: {last_topic}\")\n",
    "\n",
    "                    # If subtopic exists in row\n",
    "                    if len(row) > 1 and row[1] != \"\":\n",
    "                        subtopic = row[1]\n",
    "                        if verbose:\n",
    "                            print(f\"\\tInside subtopic: {subtopic}\")\n",
    "\n",
    "                    for item in row:\n",
    "                        if validators.url(item):\n",
    "                            index += 1\n",
    "                            range_count += 1\n",
    "\n",
    "                            if (content := read_from_link(item)) == \"\":\n",
    "                                unscraped.append(item)\n",
    "                                continue\n",
    "                            else:\n",
    "                                check_f.write(item)\n",
    "                                check_f.write(\"\\n\")\n",
    "                                check_f.write(content)\n",
    "                                check_f.write(\"\\n\\n\")\n",
    "\n",
    "                            data = {\n",
    "                                \"topic\": last_topic,\n",
    "                                \"subtopic\": subtopic,\n",
    "                                \"url\": item,\n",
    "                                \"data\": content,\n",
    "                            }\n",
    "\n",
    "                            if subtopic:\n",
    "                                if not os.path.exists(\n",
    "                                    os.path.join(folder_name, last_topic, subtopic)\n",
    "                                ):\n",
    "                                    os.makedirs(\n",
    "                                        os.path.join(folder_name, last_topic, subtopic)\n",
    "                                    )\n",
    "                                with open(\n",
    "                                    os.path.join(\n",
    "                                        folder_name,\n",
    "                                        last_topic,\n",
    "                                        subtopic,\n",
    "                                        f\"{index}.json\",\n",
    "                                    ),\n",
    "                                    \"w\",\n",
    "                                    encoding=\"utf-8\",\n",
    "                                ) as f:\n",
    "                                    json.dump(data, f)\n",
    "                            else:\n",
    "                                if not os.path.exists(\n",
    "                                    os.path.join(folder_name, last_topic)\n",
    "                                ):\n",
    "                                    os.makedirs(os.path.join(folder_name, last_topic))\n",
    "                                with open(\n",
    "                                    os.path.join(\n",
    "                                        folder_name, last_topic, f\"{index}.json\"\n",
    "                                    ),\n",
    "                                    \"w\",\n",
    "                                    encoding=\"utf-8\",\n",
    "                                ) as f:\n",
    "                                    json.dump(data, f)\n",
    "\n",
    "            print(f\"\\nTotal links in {single_range['range']}:\\t{range_count}\\n\")\n",
    "            link_count += range_count\n",
    "\n",
    "        print(f\"\\nTotal links in sheet:\\t{link_count}\")\n",
    "\n",
    "    check_f.close()\n",
    "\n",
    "    with open(\"unscraped.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for link in unscraped:\n",
    "            f.write(link)\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5d17c5e4-5c44-45b2-959d-8ff30ca06791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 ranges retrieved\n",
      "\n",
      "In range AGRI\n",
      "\n",
      "\n",
      "Total links in AGRICULTURE!C1:S1236:\t358\n",
      "\n",
      "In range FIN\n",
      "\n",
      "\n",
      "Total links in FINANCE!C1:S1234:\t234\n",
      "\n",
      "In range GEN\n",
      "\n",
      "\n",
      "Total links in GENERAL!C1:S1943:\t820\n",
      "\n",
      "\n",
      "Total links in sheet:\t1412\n"
     ]
    }
   ],
   "source": [
    "extract([\"AGRI\", \"FIN\", \"GEN\"], verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0726a90f-c281-463f-843d-3beebd907e1b",
   "metadata": {},
   "source": [
    "## Verifying JSON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7aa7ba13-ec8a-4d74-bdeb-d45b6c400fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'Agricultural education',\n",
       " 'subtopic': '',\n",
       " 'url': 'https://hanka.gurturgoth.com/krishi-vigyan-kendra/',\n",
       " 'data': 'बेमेतरा 19 मार्च 2021। कृषि विज्ञान केन्द्र, कृषि महाविद्यालय अउ अनुसंधान केन्द्र ढोलिया बेमेतरा के सापर तत्वधान म बिरस्पत 18 मार्च के जिला स्तरीय किसान मेला सह संगोष्ठी के आयोजन करे गइस। मेला के मुख्य उद्देश्य अलसी व दलहनी फसल अउ खरीफ/रबी फसल मन के बीज उत्पादन ल प्रोत्साहित करना रिहिन। (मेला अखिल भारतीय समन्वित अलसी अनुसंधान परियोजना, अखिल भारतीय समन्वित मुलार्प अनुसंधान परियोजना अउ राष्ट्रीय बीज परियोजना- मेगा सीड परियोजना डाहर ले प्रायोजित रिहिन।) कार्यक्रम म मुख्य अतिथि कृषि मंत्री श्री रविन्द्र चौबे  विशिष्ट अतिथि विधायक बेमेतरा श्री आशीष छाबड़ा, डा. एस. के. पाटील (कुलपति इंदिरागांधी कृषि विश्वविद्यालय रायपुर) डा. एस. सी. मुखर्जी निदेशक विस्तार सेवायें, इंदिरागांधी कृषि विश्वविद्यालय रायपुर, डा. आर. के. द्विवेदी अधिष्ठाता कृषि महाविद्यालय अउ अनुसंधान केन्द्र, कवर्धा, डाॅ. डी. एस. ठाकुर अधिष्ठाता कृषि महाविद्यालय अउ अनुसंधान केन्द्र, साजा, बंशी पटेल, श्रीमति प्रज्ञा निर्वाणी (जिला पंचायत सदस्य बेमेतरा) के संग जिला, जनपद अउ पंचायत के आने प्रतिनिधि मन के गरिमामय उपस्थिति रहीन। जिला प्रशासन से श्री दुर्गेश वर्मा एस.डी.एम., उपसंचालक कृषि श्री एम. डी. मानकर, डाॅ. के पी वर्मा अधिष्ठाता कृषि माहाविद्यालय ढोलिया (बेमेतरा), एस.डी.ओ. सोलंकी शर्मा अउ जम्मो  ब्लाक के एस.ए. डी.ओ./आर. ए. इ.ओ. उप संचालक उपस्थित रिहिन।  कृषि मंत्री श्री रविन्द्र चौबे डाहर ले कृषि विज्ञान केन्द्र, कृषि महाविद्यालय अउ अनुसंधान केन्द्र, बेमेतरा के काम—काज अउ उदीम मन ल सहराए गहस। संगे —संग वैज्ञानिक मन ले कृषि क्षेत्र में किसान मन ल उन्नत कृषि कोति ले जाए अउ कृषि के भरोसा सशक्तिकरण के बात कहे गईन। विधायक के द्वारा भी कृषि और कृषकों को कृषि विज्ञान केन्द्र से मिलने वाले लाभों की सराहना की। किसान मेला म कृषि उद्यानिकी, मत्स्य व पशु विभाग के सहयोग रिहिन अउ स्टाल तको लगाये गेहे रिहिन। ये बेरा सोयाबीन सीड हब-बीज भण्डार गृह के भूमि पूजन अउ एनएचएम-एमएडीएच अंतर्गत स्थापित लघु मातृ वाटिका (नान्हे नर्सरी इकाई) के लोकार्पण तको करे गहस।'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open(os.path.join(\"AGRICULTURE\", \"Agricultural education\", \"1.json\"))\n",
    "sample_data = json.load(f)\n",
    "sample_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24be3201-437e-414e-95ba-a27614dfea8a",
   "metadata": {},
   "source": [
    "## Search links from unadded, containing a particular substring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "687990c0-9052-467e-b856-6851f4bba5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3851 rows retrieved.\n"
     ]
    }
   ],
   "source": [
    "# Getting un-added links\n",
    "result = (\n",
    "    service.spreadsheets()\n",
    "    .values()\n",
    "    .get(spreadsheetId=SPREADSHEET_ID, range=UN_RANGE)\n",
    "    .execute()\n",
    ")\n",
    "unadded = result.get(\"values\", [])\n",
    "print(f\"{len(unadded)} rows retrieved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8c9670ae-509e-4b22-8e40-92944113ec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gg_links = pd.read_csv(os.path.join(\"unadded_links\", \"unadded_an_links.csv\"))\n",
    "an_links = pd.read_csv(os.path.join(\"unadded_links\", \"unadded_gg_links.csv\"))\n",
    "combined_links = pd.concat([gg_links, an_links])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fc5abe17-8149-4fa0-9261-010a1f5bdc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_links(combined_links, substring_list):\n",
    "    \"\"\"\n",
    "    Gets all links from unadded that have a particular substring in them\n",
    "    \"\"\"\n",
    "    matching_links = []\n",
    "    for link in combined_links.iloc[:, 0].tolist():\n",
    "        for substring in substring_list:\n",
    "            if substring.lower() in link.lower():\n",
    "                matching_links.append(link)\n",
    "    return matching_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "83f669d4-7d5f-4cff-a119-1e3921ca3249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.anjor.online/2021/05/bhupesh-baghel-cm-cg-kisan-naya-yoajan.html',\n",
       " 'https://www.anjor.online/2020/05/kheti-kisani.html',\n",
       " 'https://www.anjor.online/2020/05/rajiv-gandhi-kisan-nyay-yojana.html',\n",
       " 'https://www.anjor.online/2020/05/Rajiv-Gandhi-Kisan-Nyay-Yojana.html',\n",
       " 'https://www.anjor.online/2020/04/kisan.html',\n",
       " 'https://www.anjor.online/2021/03/rajiv-gandhi-kisan-nayay-yojana.html',\n",
       " 'https://www.anjor.online/2020/08/bhupesh-baghel-kisan-yojana.html',\n",
       " 'https://hanka.gurturgoth.com/rajiv-gandhi-kisan-nyay-yojana-to-be-launched-on-21st-may-in-chhattisgarh/',\n",
       " 'https://hanka.gurturgoth.com/kondagaon-raghuram-kisan/',\n",
       " 'https://hanka.gurturgoth.com/kisani-ke-goth/',\n",
       " 'https://hanka.gurturgoth.com/sabal-kisan/',\n",
       " 'https://hanka.gurturgoth.com/during-the-lockdown-the-bhoomgadi-mahila-kisan-group-served-food-access-to-the-house/',\n",
       " 'https://hanka.gurturgoth.com/rajiv-gandhi-kisan-nyay-yojana/',\n",
       " 'https://hanka.gurturgoth.com/kisan-nalkup-connection/',\n",
       " 'https://hanka.gurturgoth.com/kisan-sarkar-ke-pahili-prathmikata/',\n",
       " 'https://hanka.gurturgoth.com/gujarat-kisan-sahayata/',\n",
       " 'https://hanka.gurturgoth.com/krishi-vishwavidhyalaya/',\n",
       " 'https://hanka.gurturgoth.com/kisan-ke-pira/',\n",
       " 'https://hanka.gurturgoth.com/call-center-for-information-about-prime-minister-kisan-samman-nidhi/',\n",
       " 'https://hanka.gurturgoth.com/kisan-man-ke-chehara/',\n",
       " 'https://hanka.gurturgoth.com/kisan-hit-nirnay/',\n",
       " 'https://hanka.gurturgoth.com/rajiv-gandhi-kisan-nyaya-yojana-kisan-dukhut-ram-received-one-lakh-rupees-in-first-installment/']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing\n",
    "find_links(combined_links, [\"kisan\", \"krishi\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df11b56e-b6ad-49b4-8eab-0f0a5ca546b6",
   "metadata": {},
   "source": [
    "## Get links that have all topics, subtopics in them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "58510936-f217-464e-8dd5-34c70fd49d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 ranges retrieved.\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Call the Sheets API\n",
    "sheet = service.spreadsheets()\n",
    "result = sheet.values().batchGet(spreadsheetId=SPREADSHEET_ID, ranges=RANGES).execute()\n",
    "ranges = result.get(\"valueRanges\", [])\n",
    "\n",
    "sheet_links = set()\n",
    "link_count = 0\n",
    "last_topic = \"\"\n",
    "\n",
    "if not ranges:\n",
    "    print(\"No data found.\")\n",
    "else:\n",
    "    print(f\"{len(ranges)} ranges retrieved.\\n\")\n",
    "\n",
    "    # Iterate over all domains i.e Agri, Finance, General\n",
    "    for single_range in ranges:\n",
    "        range_list = set()\n",
    "\n",
    "        for i, row in enumerate(single_range[\"values\"]):\n",
    "            index = 0\n",
    "            if len(row) != 0 and i != 0:\n",
    "\n",
    "                # If topic exists in row\n",
    "                if row[0]:\n",
    "                    range_list.add(\"-\".join(row[0].strip(digits).lower().split()))\n",
    "                    # print(f\"Inside topic: {row[0]}\")\n",
    "\n",
    "                # If subtopic exists in row\n",
    "                if len(row) > 1 and row[1] != \"\":\n",
    "                    range_list.add(\"-\".join(row[1].strip(digits).lower().split()))\n",
    "                    # print(f\"\\tInside subtopic: {row[1]}\")\n",
    "\n",
    "        # Create file to store the lists\n",
    "        file_name = os.path.join(\n",
    "            \"unadded_links\",\n",
    "            \"unadded_\" + single_range[\"range\"].split(\"!\")[0].lower() + \".txt\",\n",
    "        )\n",
    "        with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "            for link in find_links(combined_links, list(range_list)):\n",
    "                f.write(link)\n",
    "                f.write(\"\\n\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48af9e09-ca32-4e4c-82c9-9a911810e69c",
   "metadata": {},
   "source": [
    "## Removing punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dea8c32a-a33a-44b5-b12c-b0e632853ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1815620"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_content = open(\n",
    "    os.path.join(\"check\", \"check_content.txt\"), \"r\", encoding=\"utf-8\"\n",
    ").read()\n",
    "len(check_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fef8b3db-bb74-41c4-ae89-48a0b3714ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    text = re.sub(r\"\\s*[a-zA-Z]\\s*\", \" \", text)  # Removing alphabets\n",
    "    # text = re.sub('[0-9]', ' ' ,text) # Removing numbers\n",
    "    # text = re.sub('[।]','. ',text) # Replacing पूर्ण विराम with full stop\n",
    "\n",
    "    # text = re.sub(\"\\.\\s\", \"। \", text)\n",
    "\n",
    "    text = re.sub(r\"\\s*’\\s*\", \"'\", text)\n",
    "    text = re.sub(r\"\\s*‘\\s*\", \"'\", text)\n",
    "    text = re.sub(r\"\\s*'\\s*\", \"'\", text)\n",
    "    text = re.sub(r\"\\s*“\\s*\", '\"', text)\n",
    "    text = re.sub(r\"\\s*”\\s*\", '\"', text)\n",
    "    text = re.sub(r'\\s*\"\\s*', '\"', text)\n",
    "\n",
    "    text = re.sub(\"\\s*\\xa0\\s*\", \" \", text)\n",
    "\n",
    "    text = re.sub(r\"\\s*[+*!?&^$|\\\\]+\\s*\", \" \", text)\n",
    "    text = re.sub(r\"\\s*[\\([{})\\]]\\s*\", \" \", text)\n",
    "    text = re.sub(r\"\\s*,\\s*\", \" \", text)\n",
    "    text = re.sub(r\"\\s*;\\s*\", \" \", text)\n",
    "    text = re.sub(r\"\\s*:\\s*\", \" \", text)\n",
    "    text = re.sub(r\"\\s*—\\s*\", \" \", text)\n",
    "    text = re.sub(r\"\\s*-\\s*\", \" \", text)\n",
    "    text = re.sub(r\"\\s*_\\s*\", \" \", text)\n",
    "    text = re.sub(r\"\\s*@\\s*\", \" \", text)\n",
    "    text = re.sub(r\"\\s*#\\s*\", \" \", text)\n",
    "    text = re.sub(r\"\\s*%\\s*\", \" \", text)\n",
    "    text = re.sub(r\"\\s*=\\s*\", \" \", text)\n",
    "    text = re.sub(r\"\\s*/\\s*\", \" \", text)\n",
    "    text = re.sub(r\"\\s*<\\s*\", \" \", text)\n",
    "    text = re.sub(r\"\\s*>\\s*\", \" \", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "27da0f21-8b38-4a4e-8cf3-7908895cade1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341951\n",
      "21943\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['.',\n",
       " '.',\n",
       " '2',\n",
       " '19',\n",
       " 'मार्च',\n",
       " '2021।',\n",
       " 'हमर',\n",
       " 'प्रदेश',\n",
       " 'के',\n",
       " 'मुखिया',\n",
       " 'दाऊ',\n",
       " 'भूपेश',\n",
       " 'बघेल',\n",
       " 'ह',\n",
       " '21']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing\n",
    "clean_content = remove_punct(check_content)\n",
    "words = clean_content.split()\n",
    "print(len(words))\n",
    "print(len(set(words)))\n",
    "words[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2b8b8f-eb96-46fb-9964-3b1430e3d475",
   "metadata": {},
   "source": [
    "## Creating word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0655b46d-8d66-4b3f-8bc9-1fe77fdbc023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count: 324879\n",
      "Number of unique words: 19731\n"
     ]
    }
   ],
   "source": [
    "# Word count for entire vocabulary\n",
    "\n",
    "dct = Counter()\n",
    "\n",
    "word_count = 0\n",
    "for word in words:\n",
    "    word = word.strip()\n",
    "    bad_words = \"।‘’“”'\\\"\"\n",
    "    word = word.translate(str.maketrans(\"\", \"\", bad_words))\n",
    "    word = word.strip(digits).strip(\".\")\n",
    "    if has_devnagri(word):\n",
    "        dct[word] = dct.get(word, 0) + 1\n",
    "        word_count += 1\n",
    "\n",
    "print(f\"Word count: {word_count}\")\n",
    "print(f\"Number of unique words: {len(dct)}\")\n",
    "\n",
    "if not os.path.exists(\"counts\"):\n",
    "    os.mkdir(\"counts\")\n",
    "\n",
    "with open(os.path.join(\"counts\", \"words.csv\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"word,count\\n\")\n",
    "    for k, v in dct.most_common():\n",
    "        f.write(f\"{k},{v}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "68f1d702-3861-4e81-b90a-d1d549d4e36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_count(domains):\n",
    "    \"\"\"\n",
    "    Creates word counts for each domain\n",
    "    \"\"\"\n",
    "    for domain in domains:\n",
    "        word_count = 0\n",
    "        count_dict = Counter()\n",
    "        for root, dirs, files in os.walk(domain, topdown=True):\n",
    "            for file in files:\n",
    "                with open(os.path.join(root, file)) as f:\n",
    "                    data = json.load(f)[\"data\"]\n",
    "                    cleaned_data = remove_punct(data)\n",
    "                    words = cleaned_data.split()\n",
    "                    for word in words:\n",
    "                        word = word.strip()\n",
    "                        bad_words = \"।‘’“”'\\\"\"\n",
    "                        word = word.translate(str.maketrans(\"\", \"\", bad_words))\n",
    "                        word = word.strip(digits).strip(\".\")\n",
    "                        if has_devnagri(word):\n",
    "                            count_dict[word] = count_dict.get(word, 0) + 1\n",
    "                            word_count += 1\n",
    "        if not os.path.exists(\"counts\"):\n",
    "            os.mkdir(\"counts\")\n",
    "        file_name = os.path.join(\"counts\", domain.lower() + \"_count.csv\")\n",
    "        print(f\"Word count for {domain} is {word_count}\")\n",
    "        with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"word,count\\n\")\n",
    "            for k, v in count_dict.most_common():\n",
    "                f.write(f\"{k},{v}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "de645054-bd85-4ca9-ab6a-882c66581551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count for AGRICULTURE is 88144\n",
      "Word count for FINANCE is 64536\n",
      "Word count for GENERAL is 169028\n"
     ]
    }
   ],
   "source": [
    "create_word_count([\"AGRICULTURE\", \"FINANCE\", \"GENERAL\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de81d88-18fe-45a5-a0e9-afcd22a2bbb3",
   "metadata": {},
   "source": [
    "### Test: Check the number of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3cb0a3e9-6db6-46d2-b30f-e97f4b6d5ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of files in the domains\n",
    "def file_count(domains):\n",
    "    links = []\n",
    "    for domain in domains:\n",
    "        file_count = 0\n",
    "        for root, dirs, files in os.walk(domain):\n",
    "            file_count += len(files)\n",
    "            for file in files:\n",
    "                with open(os.path.join(root, file)) as f:\n",
    "                    links.append(json.load(f)[\"url\"])\n",
    "        print(f\"{domain}: {file_count}\")\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6abc675a-7378-44eb-9fec-fdce62f10ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINANCE: 228\n"
     ]
    }
   ],
   "source": [
    "fin_links = file_count([\"FINANCE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ec227cbe-dd8f-46e3-9277-cb98a154f1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "415 rows retrieved.\n"
     ]
    }
   ],
   "source": [
    "# Getting finance extra links\n",
    "result = (\n",
    "    service.spreadsheets()\n",
    "    .values()\n",
    "    .get(spreadsheetId=SPREADSHEET_ID, range=FIN_RANGE)\n",
    "    .execute()\n",
    ")\n",
    "fins = result.get(\"values\", [])\n",
    "print(f\"{len(fins)} rows retrieved.\")\n",
    "\n",
    "fin_links_sheet = []\n",
    "for row in fins:\n",
    "    for cell in row:\n",
    "        if validators.url(cell):\n",
    "            fin_links_sheet.append(cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2e2073f6-f6c8-4f12-8472-5465f7d57e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228 234\n"
     ]
    }
   ],
   "source": [
    "print(len(fin_links), len(fin_links_sheet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e088ed3b-4fa4-4e45-b7f0-bf0def0eb3e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://36garhi.com/2021/05/26/%e0%a4%b8%e0%a5%8d%e0%a4%9f%e0%a5%80%e0%a4%b2-%e0%a4%89%e0%a4%a6%e0%a5%8d%e0%a4%af%e0%a5%8b%e0%a4%97-%e0%a4%ac%e0%a4%b0-%e0%a4%9b%e0%a4%a4%e0%a5%8d%e0%a4%a4%e0%a5%80%e0%a4%b8%e0%a4%97%e0%a5%9d/',\n",
       " 'http://hanka.gurturgoth.com/%E0%A4%AE%E0%A5%81%E0%A4%82%E0%A4%97%E0%A5%87%E0%A4%B2%E0%A5%80-%E0%A4%95%E0%A5%87-%E0%A4%97%E0%A4%BE%E0%A4%82%E0%A4%B5-%E0%A4%AE%E0%A4%A8%E0%A4%95%E0%A5%80-%E0%A4%B8%E0%A5%8D%E0%A4%A5%E0%A4%BF/',\n",
       " 'https://morchhattisgarhia.wordpress.com/2018/07/17/%E0%A4%B5%E0%A4%BF%E0%A4%95%E0%A4%BE%E0%A4%B8-%E0%A4%95%E0%A5%87-%E0%A4%AA%E0%A4%A5-%E0%A4%A4%E0%A5%87%E0%A4%9C%E0%A5%80-%E0%A4%B8%E0%A5%87-%E0%A4%A6%E0%A5%8C%E0%A5%9C%E0%A4%A4%E0%A5%87-%E0%A4%9B/',\n",
       " 'https://morchhattisgarhia.wordpress.com/2018/05/10/%E0%A4%AC%E0%A5%87%E0%A4%B9%E0%A4%A4%E0%A4%B0-%E0%A4%B8%E0%A5%8D%E0%A4%B5%E0%A4%BE%E0%A4%B8%E0%A5%8D%E0%A4%A5%E0%A5%8D%E0%A4%AF-%E0%A4%B8%E0%A5%87%E0%A4%B5%E0%A4%BE%E0%A4%93%E0%A4%82-%E0%A4%B8/',\n",
       " 'http://hanka.gurturgoth.com/%E0%A4%AE%E0%A5%81%E0%A4%96%E0%A5%8D%E0%A4%AF%E0%A4%AE%E0%A4%82%E0%A4%A4%E0%A5%8D%E0%A4%B0%E0%A5%80-%E0%A4%B6%E0%A5%8D%E0%A4%B0%E0%A5%80-%E0%A4%AD%E0%A5%82%E0%A4%AA%E0%A5%87%E0%A4%B6-%E0%A4%AC-7/',\n",
       " 'https://36garhi.com/2021/05/02/%e0%a4%95%e0%a4%b0%e0%a5%8d%e0%a4%ae%e0%a4%9a%e0%a4%be%e0%a4%b0%e0%a5%80-%e0%a4%b0%e0%a4%be%e0%a4%9c%e0%a5%8d%e0%a4%af-%e0%a4%ac%e0%a5%80%e0%a4%ae%e0%a4%be-%e0%a4%b8%e0%a5%87%e0%a4%b5%e0%a4%be/']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(fin_links_sheet) - set(fin_links))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65449df6-9ae6-46d8-b610-800478f0047e",
   "metadata": {},
   "source": [
    "## Separating Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "71b96435-de7e-44c4-b0eb-58b1d6a86b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "test_string = \"बेमेतरा 19 मार्च 2021। कृषि विज्ञान केन्द्र, कृषि महाविद्यालय अउ अनुसंधान केन्द्र ढोलिया बेमेतरा के सापर तत्वधान म बिरस्पत 18 मार्च के जिला स्तरीय किसान मेला सह संगोष्ठी के आयोजन करे गइस। मेला के मुख्य उद्देश्य अलसी व दलहनी फसल अउ खरीफ/रबी फसल मन के बीज उत्पादन ल प्रोत्साहित करना रिहिन। (मेला अखिल भारतीय समन्वित अलसी अनुसंधान परियोजना, अखिल भारतीय समन्वित मुलार्प अनुसंधान परियोजना अउ राष्ट्रीय बीज परियोजना- मेगा सीड परियोजना डाहर ले प्रायोजित रिहिन।) कार्यक्रम म मुख्य अतिथि कृषि मंत्री श्री रविन्द्र चौबे  विशिष्ट अतिथि विधायक बेमेतरा श्री आशीष छाबड़ा, डा. एस. के. पाटील (कुलपति इंदिरागांधी कृषि विश्वविद्यालय रायपुर) डा. एस. सी. मुखर्जी निदेशक विस्तार सेवायें, इंदिरागांधी कृषि विश्वविद्यालय रायपुर, डा. आर. के. द्विवेदी अधिष्ठाता कृषि महाविद्यालय अउ अनुसंधान केन्द्र, कवर्धा, डाॅ. डी. एस. ठाकुर अधिष्ठाता कृषि महाविद्यालय अउ अनुसंधान केन्द्र, साजा, बंशी पटेल, श्रीमति प्रज्ञा निर्वाणी (जिला पंचायत सदस्य बेमेतरा) के संग जिला, जनपद अउ पंचायत के आने प्रतिनिधि मन के गरिमामय उपस्थिति रहीन। जिला प्रशासन से श्री दुर्गेश वर्मा एस.डी.एम., उपसंचालक कृषि श्री एम. डी. मानकर, डाॅ. के पी वर्मा अधिष्ठाता कृषि माहाविद्यालय ढोलिया (बेमेतरा), एस.डी.ओ. सोलंकी शर्मा अउ जम्मो  ब्लाक के एस.ए. डी.ओ./आर. ए. इ.ओ. उप संचालक उपस्थित रिहिन।  कृषि मंत्री श्री रविन्द्र चौबे डाहर ले कृषि विज्ञान केन्द्र, कृषि महाविद्यालय अउ अनुसंधान केन्द्र, बेमेतरा के काम—काज अउ उदीम मन ल सहराए गहस। संगे —संग वैज्ञानिक मन ले कृषि क्षेत्र में किसान मन ल उन्नत कृषि कोति ले जाए अउ कृषि के भरोसा सशक्तिकरण के बात कहे गईन। विधायक के द्वारा भी कृषि और कृषकों को कृषि विज्ञान केन्द्र से मिलने वाले लाभों की सराहना की। किसान मेला म कृषि उद्यानिकी, मत्स्य व पशु विभाग के सहयोग रिहिन अउ स्टाल तको लगाये गेहे रिहिन। ये बेरा सोयाबीन सीड हब-बीज भण्डार गृह के भूमि पूजन अउ एनएचएम-एमएडीएच अंतर्गत स्थापित लघु मातृ वाटिका (नान्हे नर्सरी इकाई) के लोकार्पण तको करे गहस।\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ee3e692c-62fa-4b89-a648-6b6db3e188d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'बेमेतरा 19 मार्च 2021। कृषि विज्ञान केन्द्र कृषि महाविद्यालय अउ अनुसंधान केन्द्र ढोलिया बेमेतरा के सापर तत्वधान म बिरस्पत 18 मार्च के जिला स्तरीय किसान मेला सह संगोष्ठी के आयोजन करे गइस। मेला के मुख्य उद्देश्य अलसी व दलहनी फसल अउ खरीफ रबी फसल मन के बीज उत्पादन ल प्रोत्साहित करना रिहिन। मेला अखिल भारतीय समन्वित अलसी अनुसंधान परियोजना अखिल भारतीय समन्वित मुलार्प अनुसंधान परियोजना अउ राष्ट्रीय बीज परियोजना मेगा सीड परियोजना डाहर ले प्रायोजित रिहिन। कार्यक्रम म मुख्य अतिथि कृषि मंत्री श्री रविन्द्र चौबे  विशिष्ट अतिथि विधायक बेमेतरा श्री आशीष छाबड़ा डा. एस. के. पाटील कुलपति इंदिरागांधी कृषि विश्वविद्यालय रायपुर डा. एस. सी. मुखर्जी निदेशक विस्तार सेवायें इंदिरागांधी कृषि विश्वविद्यालय रायपुर डा. आर. के. द्विवेदी अधिष्ठाता कृषि महाविद्यालय अउ अनुसंधान केन्द्र कवर्धा डाॅ. डी. एस. ठाकुर अधिष्ठाता कृषि महाविद्यालय अउ अनुसंधान केन्द्र साजा बंशी पटेल श्रीमति प्रज्ञा निर्वाणी जिला पंचायत सदस्य बेमेतरा के संग जिला जनपद अउ पंचायत के आने प्रतिनिधि मन के गरिमामय उपस्थिति रहीन। जिला प्रशासन से श्री दुर्गेश वर्मा एस.डी.एम. उपसंचालक कृषि श्री एम. डी. मानकर डाॅ. के पी वर्मा अधिष्ठाता कृषि माहाविद्यालय ढोलिया बेमेतरा एस.डी.ओ. सोलंकी शर्मा अउ जम्मो  ब्लाक के एस.ए. डी.ओ. आर. ए. इ.ओ. उप संचालक उपस्थित रिहिन।  कृषि मंत्री श्री रविन्द्र चौबे डाहर ले कृषि विज्ञान केन्द्र कृषि महाविद्यालय अउ अनुसंधान केन्द्र बेमेतरा के काम काज अउ उदीम मन ल सहराए गहस। संगे संग वैज्ञानिक मन ले कृषि क्षेत्र में किसान मन ल उन्नत कृषि कोति ले जाए अउ कृषि के भरोसा सशक्तिकरण के बात कहे गईन। विधायक के द्वारा भी कृषि और कृषकों को कृषि विज्ञान केन्द्र से मिलने वाले लाभों की सराहना की। किसान मेला म कृषि उद्यानिकी मत्स्य व पशु विभाग के सहयोग रिहिन अउ स्टाल तको लगाये गेहे रिहिन। ये बेरा सोयाबीन सीड हब बीज भण्डार गृह के भूमि पूजन अउ एनएचएम एमएडीएच अंतर्गत स्थापित लघु मातृ वाटिका नान्हे नर्सरी इकाई के लोकार्पण तको करे गहस।'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "cleaned_test_string = remove_punct(test_string)\n",
    "cleaned_test_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8b89bd0a-25d4-4c28-90a8-733abf3f0184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentences(contents):\n",
    "    \"\"\"\n",
    "    Get a list of cleaned sentences\n",
    "    \"\"\"\n",
    "    cleaned_sentences = []\n",
    "    for sentence in re.split(\"[।\\n]\", contents):\n",
    "        cleaned_sentence = remove_punct(sentence).strip()\n",
    "        if 8 < len(cleaned_sentence) < 1024 and has_devnagri(cleaned_sentence):\n",
    "            cleaned_sentences.append(cleaned_sentence)\n",
    "    return cleaned_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eb149ac5-5937-4dbe-bc8d-10e9d9e9e618",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"sentences\"):\n",
    "    os.mkdir(\"sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "57309e64-03ea-4644-96b5-c0d79675146d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines in file: 483728\n",
      "Number of sentences in file: 3939\n"
     ]
    }
   ],
   "source": [
    "agri_check = open(os.path.join(\"check\", \"agri_check.txt\"), \"r\", encoding=\"utf-8\").read()\n",
    "agri_sent = clean_sentences(agri_check)\n",
    "\n",
    "print(f\"Number of lines in file: {len(agri_check)}\")\n",
    "print(f\"Number of sentences in file: {len(agri_sent)}\")\n",
    "\n",
    "with open(os.path.join(\"sentences\", \"agri.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    for sentence in agri_sent:\n",
    "        f.write(f\"{sentence}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ed2ccc0e-079f-445e-8743-e5c94a179e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines in file: 356831\n",
      "Number of sentences in file: 2786\n"
     ]
    }
   ],
   "source": [
    "fin_check = open(os.path.join(\"check\", \"fin_check.txt\"), \"r\", encoding=\"utf-8\").read()\n",
    "fin_sent = clean_sentences(fin_check)\n",
    "\n",
    "print(f\"Number of lines in file: {len(fin_check)}\")\n",
    "print(f\"Number of sentences in file: {len(fin_sent)}\")\n",
    "\n",
    "with open(os.path.join(\"sentences\", \"fin.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    for sentence in fin_sent:\n",
    "        f.write(f\"{sentence}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3dabbdca-3bc4-4547-a150-57c76241c7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines in file: 975057\n",
      "Number of sentences in file: 7991\n"
     ]
    }
   ],
   "source": [
    "gen_check = open(os.path.join(\"check\", \"gen_check.txt\"), \"r\", encoding=\"utf-8\").read()\n",
    "gen_sent = clean_sentences(gen_check)\n",
    "\n",
    "print(f\"Number of lines in file: {len(gen_check)}\")\n",
    "print(f\"Number of sentences in file: {len(gen_sent)}\")\n",
    "\n",
    "with open(os.path.join(\"sentences\", \"gen.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    for sentence in gen_sent:\n",
    "        f.write(f\"{sentence}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bb9571c5-45c8-406d-8fa5-27e3b00ffa5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines in file: 1815620\n",
      "Number of sentences in file: 14716\n"
     ]
    }
   ],
   "source": [
    "all_check = open(\n",
    "    os.path.join(\"check\", \"check_content.txt\"), \"r\", encoding=\"utf-8\"\n",
    ").read()\n",
    "all_sent = clean_sentences(all_check)\n",
    "\n",
    "print(f\"Number of lines in file: {len(all_check)}\")\n",
    "print(f\"Number of sentences in file: {len(all_sent)}\")\n",
    "\n",
    "with open(os.path.join(\"sentences\", \"all.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    for sentence in all_sent:\n",
    "        f.write(f\"{sentence}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7132a66-9fbc-40d1-941b-a7da90cfd462",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
