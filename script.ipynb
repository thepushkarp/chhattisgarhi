{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1193d97-f487-43ea-abbc-88b9640c390e",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6adb6d7b-71ce-4563-ac09-b27060f737a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import validators\n",
    "from urllib.parse import urlparse\n",
    "import json\n",
    "import string\n",
    "from string import digits\n",
    "from collections import Counter\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ec89c318-e3a9-4f7a-9902-6ca992412268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaee84a8-b38e-4b46-a941-ba3b02390d4e",
   "metadata": {},
   "source": [
    "# Initializing the Google Sheets API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2987688e-3166-49cb-9bf2-8354d1704db7",
   "metadata": {},
   "source": [
    "**NOTE**: Use https://developers.google.com/sheets/api/quickstart/python to first create a Google Cloud project and enable the Google Sheets API. Then download the `credentials.json` to the same directory as this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0de3da55-a05d-46f9-9431-68aa6ad40e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ID and range of a sample spreadsheet.\n",
    "SPREADSHEET_ID = os.getenv(\"SPREADSHEET_ID\")\n",
    "GENERAL_RANGE = \"GENERAL!C:S\"\n",
    "AGRI_RANGE = \"AGRICULTURE!C:S\"\n",
    "FIN_RANGE = \"FINANCE!C:S\"\n",
    "DUP_RANGE = \"DUPLICATES!A1:A1005\"\n",
    "UN_RANGE = \"UNADDED!A:B\"\n",
    "EX_RANGE = \"EXTRA!A1:A10001\"\n",
    "RANGES = [AGRI_RANGE, FIN_RANGE, GENERAL_RANGE]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810ec98f-54b5-4551-b3f8-4ae6545d33fc",
   "metadata": {},
   "source": [
    "# Accessing Sheets, finding duplicates and unadded links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3247141-ecae-4dd2-b6c3-59e89402f348",
   "metadata": {},
   "source": [
    "## Generating token to access the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c0a63e51-eb56-49a2-aa73-17b0b83dce0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If modifying these scopes, delete the file write-token.json and read-token.json\n",
    "\n",
    "# SCOPES = [\"https://www.googleapis.com/auth/spreadsheets.readonly\"] # For read-only scope\n",
    "SCOPES = [\"https://www.googleapis.com/auth/spreadsheets\"]  # For writing to sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5c6000c7-b728-4156-88ce-78262136790a",
   "metadata": {},
   "outputs": [],
   "source": [
    "creds = None\n",
    "\n",
    "if os.path.exists(\"write-token.json\"):\n",
    "    creds = Credentials.from_authorized_user_file(\"write-token.json\", SCOPES)\n",
    "# If there are no (valid) credentials available, let the user log in.\n",
    "if not creds or not creds.valid:\n",
    "    if creds and creds.expired and creds.refresh_token:\n",
    "        creds.refresh(Request())\n",
    "    else:\n",
    "        flow = InstalledAppFlow.from_client_secrets_file(\"credentials.json\", SCOPES)\n",
    "        creds = flow.run_local_server(port=0)\n",
    "    # Save the credentials for the next run\n",
    "    with open(\"write-token.json\", \"w\") as token:\n",
    "        token.write(creds.to_json())\n",
    "\n",
    "service = build(\"sheets\", \"v4\", credentials=creds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0971fae-2ab5-48ab-af3d-53a5b728d08b",
   "metadata": {},
   "source": [
    "## Getting Sitemaps from Gurturgoth and Anjor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "33718d98-478c-4cb7-beb7-6fbe881e0079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://hanka.gurturgoth.com/post-sitemap1.xml',\n",
       " 'https://hanka.gurturgoth.com/post-sitemap2.xml',\n",
       " 'https://hanka.gurturgoth.com/post-sitemap3.xml',\n",
       " 'https://hanka.gurturgoth.com/post-sitemap4.xml',\n",
       " 'https://hanka.gurturgoth.com/post-sitemap5.xml',\n",
       " 'https://hanka.gurturgoth.com/post-sitemap6.xml']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gurturgoth Sitemaps\n",
    "gg_sitemaps = [\n",
    "    f\"https://hanka.gurturgoth.com/post-sitemap{i}.xml\" for i in range(1, 6 + 1)\n",
    "]\n",
    "gg_sitemaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "3501581c-c7d8-44b6-97d8-f1a4d628cc82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.anjor.online/sitemap.xml?page=1',\n",
       " 'https://www.anjor.online/sitemap.xml?page=2',\n",
       " 'https://www.anjor.online/sitemap.xml?page=3',\n",
       " 'https://www.anjor.online/sitemap.xml?page=4',\n",
       " 'https://www.anjor.online/sitemap.xml?page=5',\n",
       " 'https://www.anjor.online/sitemap.xml?page=6',\n",
       " 'https://www.anjor.online/sitemap.xml?page=7',\n",
       " 'https://www.anjor.online/sitemap.xml?page=8',\n",
       " 'https://www.anjor.online/sitemap.xml?page=9',\n",
       " 'https://www.anjor.online/sitemap.xml?page=10',\n",
       " 'https://www.anjor.online/sitemap.xml?page=11',\n",
       " 'https://www.anjor.online/sitemap.xml?page=12',\n",
       " 'https://www.anjor.online/sitemap.xml?page=13']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Anjor Sitemaps\n",
    "an_sitemaps = [\n",
    "    f\"https://www.anjor.online/sitemap.xml?page={i}\" for i in range(1, 13 + 1)\n",
    "]\n",
    "an_sitemaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "512e645a-ccd9-4cff-aed7-e1e35fb8be54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sitemap_links(sitemaps):\n",
    "    \"\"\"\n",
    "    Returns all the URLs found in the sitemaps\n",
    "    \"\"\"\n",
    "\n",
    "    all_urls = set()\n",
    "    header = {\n",
    "        \"Accept\": \"*/*\",\n",
    "        \"Accept-Language\": \"*\",\n",
    "        \"Accept-Encoding\": \"identity, gzip, deflate, compress, br\",\n",
    "        \"User-Agent\": \"XY\",\n",
    "    }\n",
    "\n",
    "    for sitemap in sitemaps:\n",
    "        r = requests.get(sitemap, headers=header)\n",
    "        xml = r.text\n",
    "\n",
    "        soup = BeautifulSoup(xml)\n",
    "        URLTags = soup.find_all(\"url\")\n",
    "\n",
    "        print(f\"{sitemap}: {len(URLTags)} urls found\")\n",
    "\n",
    "        for URL in URLTags:\n",
    "            all_urls.add(URL.findNext(\"loc\").text)\n",
    "\n",
    "    print(f\"Total: {len(all_urls)} links found\")\n",
    "\n",
    "    return list(all_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a30c3ba9-457a-4d78-8115-2ac23dd4c027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://hanka.gurturgoth.com/post-sitemap1.xml: 1000 urls found\n",
      "https://hanka.gurturgoth.com/post-sitemap2.xml: 1000 urls found\n",
      "https://hanka.gurturgoth.com/post-sitemap3.xml: 1000 urls found\n",
      "https://hanka.gurturgoth.com/post-sitemap4.xml: 1000 urls found\n",
      "https://hanka.gurturgoth.com/post-sitemap5.xml: 1000 urls found\n",
      "https://hanka.gurturgoth.com/post-sitemap6.xml: 66 urls found\n",
      "Total: 5066 links found\n"
     ]
    }
   ],
   "source": [
    "gg_links = get_sitemap_links(gg_sitemaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "c8a91e51-f202-4724-88f6-7eb80f99a02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.anjor.online/sitemap.xml?page=1: 150 urls found\n",
      "https://www.anjor.online/sitemap.xml?page=2: 150 urls found\n",
      "https://www.anjor.online/sitemap.xml?page=3: 150 urls found\n",
      "https://www.anjor.online/sitemap.xml?page=4: 150 urls found\n",
      "https://www.anjor.online/sitemap.xml?page=5: 150 urls found\n",
      "https://www.anjor.online/sitemap.xml?page=6: 150 urls found\n",
      "https://www.anjor.online/sitemap.xml?page=7: 150 urls found\n",
      "https://www.anjor.online/sitemap.xml?page=8: 150 urls found\n",
      "https://www.anjor.online/sitemap.xml?page=9: 150 urls found\n",
      "https://www.anjor.online/sitemap.xml?page=10: 150 urls found\n",
      "https://www.anjor.online/sitemap.xml?page=11: 150 urls found\n",
      "https://www.anjor.online/sitemap.xml?page=12: 150 urls found\n",
      "https://www.anjor.online/sitemap.xml?page=13: 71 urls found\n",
      "Total: 1871 links found\n"
     ]
    }
   ],
   "source": [
    "an_links = get_sitemap_links(an_sitemaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5d803a-d644-48c6-b3ed-a52a6bb9209b",
   "metadata": {},
   "source": [
    "## Save the sitemaps as CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f468cfe7-f7b9-4819-9d0e-aa497672d7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_csv(filename, links, foldername=\"\"):\n",
    "    \"\"\"\n",
    "    Saves a list of links as CSV files\n",
    "    filename: Name of the file to save the csv as\n",
    "    links: a collection of links to save in the csv\n",
    "    foldername: Name of the folder to save the file\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(data={\"links\": list(links)})\n",
    "    if foldername:\n",
    "        if not os.path.exists(foldername):\n",
    "            os.mkdir(foldername)\n",
    "        df.to_csv(os.path.join(foldername, filename), sep=\",\", index=False)\n",
    "    else:\n",
    "        df.to_csv(filename, sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "d5c15728-7c8c-4924-aa9d-745a895263ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_csv(\"anjor.csv\", an_links, \"sitemaps\")\n",
    "save_as_csv(\"gurtur.csv\", gg_links, \"sitemaps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124aabb7-8afc-4df0-89d9-b010024001a5",
   "metadata": {},
   "source": [
    "## Get the data summary from Google Sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2acde233-52fc-452a-aa18-4c93e85705d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 ranges retrieved.\n",
      "Total links in AGRICULTURE!C1:S1236:\t357\n",
      "Total links in FINANCE!C1:S1234:\t232\n",
      "Total links in GENERAL!C1:S1943:\t820\n",
      "\n",
      "Total links in sheet:\t1409\n",
      "Unique links in sheet:\t1409\n",
      "Duplicate links:\t0\n"
     ]
    }
   ],
   "source": [
    "# Call the Sheets API\n",
    "sheet = service.spreadsheets()\n",
    "result = sheet.values().batchGet(spreadsheetId=SPREADSHEET_ID, ranges=RANGES).execute()\n",
    "ranges = result.get(\"valueRanges\", [])\n",
    "\n",
    "sheet_links = set()\n",
    "duplicate_links = set()\n",
    "link_count = 0\n",
    "\n",
    "if not ranges:\n",
    "    print(\"No data found.\")\n",
    "else:\n",
    "    print(f\"{len(ranges)} ranges retrieved.\")\n",
    "    for single_range in ranges:\n",
    "        range_count = 0\n",
    "        for row in single_range[\"values\"]:\n",
    "            if len(row) != 0:\n",
    "                for item in row:\n",
    "                    if validators.url(item):\n",
    "                        range_count += 1\n",
    "                        if item in sheet_links:\n",
    "                            duplicate_links.add(item)\n",
    "                        else:\n",
    "                            sheet_links.add(item)\n",
    "        print(f\"Total links in {single_range['range']}:\\t{range_count}\")\n",
    "        link_count += range_count\n",
    "    print()\n",
    "    print(f\"Total links in sheet:\\t{link_count}\")\n",
    "    print(f\"Unique links in sheet:\\t{len(sheet_links)}\")\n",
    "    print(f\"Duplicate links:\\t{len(duplicate_links)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "fe8b9040-4754-4498-b799-f21f5fb3a3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Verify duplicates\n",
    "count = 0\n",
    "for link in list(duplicate_links):\n",
    "    if link in sheet_links:\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f2bca178-3194-4fa8-9e7b-3f669b4b4fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the duplicates\n",
    "save_as_csv(\"duplicates.csv\", duplicate_links, \"duplicates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9484f9-2539-4883-8f1f-febe1b9e604a",
   "metadata": {},
   "source": [
    "## Writing duplicates to Google Sheets [Caution: Can overwrite to sheet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4a57843c-699a-47e6-97af-bfe3cf9342a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing to Google Sheets\n",
    "values = [[\"DUPLICATES\"]]\n",
    "\n",
    "for value in list(duplicate_links):\n",
    "    values.append([value])\n",
    "\n",
    "len_values = len(values)\n",
    "\n",
    "for _ in range(len_values, 1000 + 1):\n",
    "    values.append([\"\"])\n",
    "\n",
    "body = {\"values\": values}\n",
    "\n",
    "value_input_option = \"USER_ENTERED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0d7e5ec3-22c4-41de-b5fa-3f2b955877e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001 cells updated.\n"
     ]
    }
   ],
   "source": [
    "result = (\n",
    "    service.spreadsheets()\n",
    "    .values()\n",
    "    .update(\n",
    "        spreadsheetId=SPREADSHEET_ID,\n",
    "        range=DUP_RANGE,\n",
    "        valueInputOption=value_input_option,\n",
    "        body=body,\n",
    "    )\n",
    "    .execute()\n",
    ")\n",
    "print(f\"{result.get('updatedCells')} cells updated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2804bf1-88d1-4917-b150-22baf63c3cc9",
   "metadata": {},
   "source": [
    "## Finding links Gurturgoth and Anjor that are not in the sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "78632c45-8ad3-4817-b891-94f47e26c045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 rows retrieved.\n"
     ]
    }
   ],
   "source": [
    "# Getting the un-needed extra links\n",
    "result = (\n",
    "    service.spreadsheets()\n",
    "    .values()\n",
    "    .get(spreadsheetId=SPREADSHEET_ID, range=EX_RANGE)\n",
    "    .execute()\n",
    ")\n",
    "extra = result.get(\"values\", [])\n",
    "print(f\"{len(extra)} rows retrieved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "5060c632-660d-48eb-9855-c95e9970abd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Gurturgoth links not in sheet: 3857\n"
     ]
    }
   ],
   "source": [
    "# Gurturgoth links not in sheet\n",
    "unadded_gg_links = []\n",
    "for link in gg_links:\n",
    "    if link not in sheet_links and link not in extra:\n",
    "        unadded_gg_links.append(link)\n",
    "\n",
    "print(f\"Number of Gurturgoth links not in sheet: {len(unadded_gg_links)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "0aa51cbe-fbba-497e-bbfe-ae1e2c50f3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Anjor links not in sheet: 1711\n"
     ]
    }
   ],
   "source": [
    "# Anjor links not in sheet\n",
    "unadded_an_links = []\n",
    "for link in an_links:\n",
    "    if link not in sheet_links and link not in extra:\n",
    "        unadded_an_links.append(link)\n",
    "\n",
    "print(f\"Number of Anjor links not in sheet: {len(unadded_an_links)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "d10de2d4-b591-4cd9-99c9-49d93a75e605",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_csv(\"unadded_gg_links.csv\", unadded_gg_links, \"unadded_links\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "939308cf-a6f0-4125-af8b-4b4d8d6331d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_csv(\"unadded_an_links.csv\", unadded_an_links, \"unadded_links\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b2e186-54d1-4c29-8b67-b43f3cc117e6",
   "metadata": {},
   "source": [
    "## Writing unadded links to Sheet [Caution: Can overwrite to sheet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "07ee2830-1d7a-45fd-9982-020280ad696f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing to Google Sheets\n",
    "values = [[\"GURTUR\", \"ANJOR\"]]\n",
    "\n",
    "for i, gg_value in enumerate(list(unadded_gg_links)):\n",
    "    an_value = \"\"\n",
    "    if i < len(unadded_an_links):\n",
    "        an_value = unadded_an_links[i]\n",
    "    values.append([gg_value, an_value])\n",
    "\n",
    "len_values = len(values)\n",
    "\n",
    "for _ in range(len_values, 5000 + 1):\n",
    "    values.append([\"\", \"\"])\n",
    "\n",
    "body = {\"values\": values}\n",
    "\n",
    "value_input_option = \"USER_ENTERED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "e562e080-86f0-4dbc-acc5-62278518f980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10002 cells updated.\n"
     ]
    }
   ],
   "source": [
    "result = (\n",
    "    service.spreadsheets()\n",
    "    .values()\n",
    "    .update(\n",
    "        spreadsheetId=SPREADSHEET_ID,\n",
    "        range=UN_RANGE,\n",
    "        valueInputOption=value_input_option,\n",
    "        body=body,\n",
    "    )\n",
    "    .execute()\n",
    ")\n",
    "print(f\"{result.get('updatedCells')} cells updated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fab91d-3c10-46aa-a6dc-324bbc3be18a",
   "metadata": {},
   "source": [
    "# Extracting data from the collected links and cleaning them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938b1207-c356-44ac-82bd-2cb5493e9f50",
   "metadata": {},
   "source": [
    "## Generating token to access the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "953c9440-5fd4-4f8a-9e7a-3b44364f6cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If modifying these scopes, delete the file write-token.json and read-token.json\n",
    "\n",
    "SCOPES = [\n",
    "    \"https://www.googleapis.com/auth/spreadsheets.readonly\"\n",
    "]  # For read-only scope\n",
    "# SCOPES = [\"https://www.googleapis.com/auth/spreadsheets\"]  # For writing to sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "186dd8d6-e1d8-4ca5-89e9-e53225aa5876",
   "metadata": {},
   "outputs": [],
   "source": [
    "creds = None\n",
    "\n",
    "if os.path.exists(\"read-token.json\"):\n",
    "    creds = Credentials.from_authorized_user_file(\"read-token.json\", SCOPES)\n",
    "# If there are no (valid) credentials available, let the user log in.\n",
    "if not creds or not creds.valid:\n",
    "    if creds and creds.expired and creds.refresh_token:\n",
    "        creds.refresh(Request())\n",
    "    else:\n",
    "        flow = InstalledAppFlow.from_client_secrets_file(\"credentials.json\", SCOPES)\n",
    "        creds = flow.run_local_server(port=0)\n",
    "    # Save the credentials for the next run\n",
    "    with open(\"read-token.json\", \"w\") as token:\n",
    "        token.write(creds.to_json())\n",
    "\n",
    "service = build(\"sheets\", \"v4\", credentials=creds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d8be87-2eb1-4e4e-b53d-c3fb5b4bcf77",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a72a60dd-e559-467a-9b79-86ca65b5dd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_gurtur(link):\n",
    "    parsed_uri = urlparse(link)\n",
    "    result = \"{uri.scheme}://{uri.netloc}/\".format(uri=parsed_uri)\n",
    "    return (\n",
    "        result == \"https://hanka.gurturgoth.com/\"\n",
    "        or result == \"http://hanka.gurturgoth.com/\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "97017eed-4077-4141-9e2f-625c3a0f199c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_anjor(link):\n",
    "    parsed_uri = urlparse(link)\n",
    "    result = \"{uri.scheme}://{uri.netloc}/\".format(uri=parsed_uri)\n",
    "    return result == \"https://www.anjor.online/\" or result == \"http://www.anjor.online/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "742f7c07-68aa-4e7d-a720-fd76392c4fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_patrika(link):\n",
    "    parsed_uri = urlparse(link)\n",
    "    result = \"{uri.scheme}://{uri.netloc}/\".format(uri=parsed_uri)\n",
    "    return result == \"https://www.patrika.com/\" or result == \"http://www.patrika.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "11d0d0b4-298d-4d37-a5a6-eff7f299f799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_36garhi(link):\n",
    "    parsed_uri = urlparse(link)\n",
    "    result = \"{uri.scheme}://{uri.netloc}/\".format(uri=parsed_uri)\n",
    "    return result == \"https://36garhi.com/\" or result == \"http://36garhi.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "32b93be0-d776-4894-8f87-6ec98a4963a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_hindi_news18(link):\n",
    "    parsed_uri = urlparse(link)\n",
    "    result = \"{uri.scheme}://{uri.netloc}/\".format(uri=parsed_uri)\n",
    "    return (\n",
    "        result == \"https://hindi.news18.com/\" or result == \"http://hindi.news18.com/\"\n",
    "    ) and parsed_uri.path.startswith(\"/news/chhattisgarhi/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e4cde293-19d2-4432-8b31-84ae7270118f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "\n",
    "assert is_gurtur(\"https://hanka.gurturgoth.com/mrs-devendra-kumari-singhdev/\") == True\n",
    "assert is_gurtur(\"http://hanka.gurturgoth.com/mrs-devendra-kumari-singhdev/\") == True\n",
    "assert is_gurtur(\"https://www.anjor.online/2021/05/baldi-bai.jpg.html\") == False\n",
    "\n",
    "assert is_anjor(\"https://www.anjor.online/2021/05/baldi-bai.jpg.html\") == True\n",
    "assert is_anjor(\"http://www.anjor.online/2021/05/baldi-bai.jpg.html\") == True\n",
    "assert is_anjor(\"https://hanka.gurturgoth.com/mrs-devendra-kumari-singhdev/\") == False\n",
    "\n",
    "assert is_patrika(\"https://hanka.gurturgoth.com/mrs-devendra-kumari-singhdev/\") == False\n",
    "assert is_patrika(\"https://www.anjor.online/2021/05/baldi-bai.jpg.html\") == False\n",
    "assert (\n",
    "    is_patrika(\"https://www.patrika.com/raipur-news/chattisgarhi-sahitya-6522552/\")\n",
    "    == True\n",
    ")\n",
    "assert (\n",
    "    is_patrika(\"http://www.patrika.com/raipur-news/chattisgarhi-sahitya-6522552/\")\n",
    "    == True\n",
    ")\n",
    "\n",
    "assert (\n",
    "    is_36garhi(\n",
    "        \"https://36garhi.com/2021/05/15/%e0%a4%b9%e0%a4%bf%e0%a4%9c%e0%a4%97%e0%a4%be/\"\n",
    "    )\n",
    "    == True\n",
    ")\n",
    "assert (\n",
    "    is_36garhi(\n",
    "        \"https://36garhi.com/2021/05/15/%e0%a4%9a%e0%a4%a8%e0%a5%8d%e0%a4%a6%e0%a5%82%e0%a4%b2%e0%a4%be%e0%a4%b2-%e0%a4%9a%e0%a4%a8%e0%a5%8d%e0%a4%a6%e0%a5%8d%e0%a4%b0%e0%a4%be%e0%a4%95%e0%a4%b0-%e0%a4%aa%e0%a5%81%e0%a4%b0%e0%a4%b8%e0%a5%8d/\"\n",
    "    )\n",
    "    == True\n",
    ")\n",
    "assert is_36garhi(\"https://www.anjor.online/2021/05/baldi-bai.jpg.html\") == False\n",
    "\n",
    "assert (\n",
    "    is_hindi_news18(\n",
    "        \"https://hindi.news18.com/news/chhattisgarhi/chhattisgarhi-epigraphy-epidemic-corona-navratri-ramleela-planetarium-astronomical-nodakm-3317016.html\"\n",
    "    )\n",
    "    == True\n",
    ")\n",
    "assert is_hindi_news18(\"https://www.anjor.online/2021/05/baldi-bai.jpg.html\") == False\n",
    "\n",
    "assert (\n",
    "    is_hindi_news18(\n",
    "        \"https://hindi.news18.com/videos/news18-virals/news18-virals-himachal-pradesh/hp-board-12th-results-meet-pushpendra-of-kullu-who-scored-500-out-of-500-in-class-12th-3656240.html?utm_source=jionews?utm_source=jionews\"\n",
    "    )\n",
    "    == False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ea0f95bc-0503-48d4-8f83-eedc566f4f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_chars = 128\n",
    "devnagri_chars = []\n",
    "for i in range(num_chars):\n",
    "    devnagri_chars.append(chr(ord(\"ऀ\") + i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "16cb755d-af0a-4d32-bf10-b13d9f030162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_devnagri(text):\n",
    "    return any(dev_char in text for dev_char in devnagri_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0ae678-a8f8-44e0-a696-416d564ae276",
   "metadata": {},
   "source": [
    "## Get data from Sheets API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1802952f-1fb0-4f05-945c-c176aae5fb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_link(link):\n",
    "    \"\"\"\n",
    "    Gets the data from page source and extracts the contents if the\n",
    "    page belongs to one of the top sites\n",
    "    \"\"\"\n",
    "    header = {\n",
    "        \"Accept\": \"*/*\",\n",
    "        \"Accept-Language\": \"*\",\n",
    "        \"Accept-Encoding\": \"identity, gzip, deflate, compress, br\",\n",
    "        \"User-Agent\": \"XY\",\n",
    "    }\n",
    "    content = \"\"\n",
    "\n",
    "    try:\n",
    "        page = requests.get(link, headers=header)\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    except:\n",
    "        return content\n",
    "\n",
    "    if is_gurtur(link):\n",
    "        if content_soup := soup.find(\"div\", class_=\"entry-content\"):\n",
    "            contents = []\n",
    "            if tags := content_soup.find_all([\"p\", \"span\", \"div\"]):\n",
    "                for tag in tags:\n",
    "                    useful_text = \"\"\n",
    "                    for text in tag.find_all(text=True):\n",
    "                        if has_devnagri(text):\n",
    "                            useful_text = \" \".join([useful_text, text.strip()])\n",
    "                    if useful_text:\n",
    "                        contents.append(useful_text)\n",
    "            if not contents:\n",
    "                for text in content_soup.find_all(text=True):\n",
    "                    useful_text = \"\"\n",
    "                    if has_devnagri(text):\n",
    "                        useful_text = \" \".join([useful_text, text.strip()])\n",
    "                        if useful_text:\n",
    "                            contents.append(useful_text)\n",
    "            content = (\" \").join(contents).strip()\n",
    "\n",
    "    elif is_anjor(link):\n",
    "        if content_soup := soup.find(\"div\", class_=\"entry-content\"):\n",
    "            contents = []\n",
    "            if tags := content_soup.find_all([\"p\", \"span\", \"div\"]):\n",
    "                for tag in tags:\n",
    "                    useful_text = \"\"\n",
    "                    for text in tag.find_all(text=True):\n",
    "                        if has_devnagri(text):\n",
    "                            useful_text = \" \".join([useful_text, text.strip()])\n",
    "                    if useful_text:\n",
    "                        contents.append(useful_text)\n",
    "            if not contents:\n",
    "                for text in content_soup.find_all(text=True):\n",
    "                    useful_text = \"\"\n",
    "                    if has_devnagri(text):\n",
    "                        useful_text = \" \".join([useful_text, text.strip()])\n",
    "                        if useful_text:\n",
    "                            contents.append(useful_text)\n",
    "            content = (\" \").join(contents).strip()\n",
    "\n",
    "    elif is_patrika(link):\n",
    "        if content_soup := soup.find(\"div\", class_=\"complete-story\"):\n",
    "            contents = []\n",
    "            if tags := content_soup.find_all([\"p\", \"span\", \"div\"]):\n",
    "                for tag in tags:\n",
    "                    useful_text = \"\"\n",
    "                    for text in tag.find_all(text=True):\n",
    "                        if has_devnagri(text):\n",
    "                            useful_text = \" \".join([useful_text, text.strip()])\n",
    "                    if useful_text:\n",
    "                        contents.append(useful_text)\n",
    "            if not contents:\n",
    "                for text in content_soup.find_all(text=True):\n",
    "                    useful_text = \"\"\n",
    "                    if has_devnagri(text):\n",
    "                        useful_text = \" \".join([useful_text, text.strip()])\n",
    "                        if useful_text:\n",
    "                            contents.append(useful_text)\n",
    "            content = (\" \").join(contents).strip()\n",
    "\n",
    "    elif is_36garhi(link):\n",
    "        if content_soup := soup.find(\"div\", class_=\"entry-content\"):\n",
    "            contents = []\n",
    "            if tags := content_soup.find_all([\"p\", \"span\", \"div\"]):\n",
    "                for tag in tags:\n",
    "                    useful_text = \"\"\n",
    "                    for text in tag.find_all(text=True):\n",
    "                        if has_devnagri(text):\n",
    "                            useful_text = \" \".join([useful_text, text.strip()])\n",
    "                    if useful_text:\n",
    "                        contents.append(useful_text)\n",
    "            if not contents:\n",
    "                for text in content_soup.find_all(text=True):\n",
    "                    useful_text = \"\"\n",
    "                    if has_devnagri(text):\n",
    "                        useful_text = \" \".join([useful_text, text.strip()])\n",
    "                        if useful_text:\n",
    "                            contents.append(useful_text)\n",
    "            content = (\" \").join(contents).strip()\n",
    "\n",
    "    elif is_hindi_news18(link):\n",
    "        if content_soup := soup.find(\"div\", class_=\"storypara\"):\n",
    "            contents = []\n",
    "            for text in content_soup.find_all(text=True):\n",
    "                useful_text = \"\"\n",
    "                if has_devnagri(text):\n",
    "                    useful_text = \"\".join([useful_text, text.strip()])\n",
    "                    if useful_text:\n",
    "                        if not \". \" in useful_text:\n",
    "                            contents.append(useful_text)\n",
    "                        else:\n",
    "                            for split_useful_text in useful_text.split(\". \"):\n",
    "                                contents.append(split_useful_text.strip(\".\") + \"।\")\n",
    "            content = (\"\").join(contents).strip()\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "53ab7ff9-461b-4a05-9627-34c23258ee7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(ranges_list=[\"AGRI\", \"FIN\", \"GEN\"], verbose=False):\n",
    "    \"\"\"\n",
    "    Extracts the links from the given domain and stores them in json files\n",
    "    \"\"\"\n",
    "\n",
    "    ranges_to_extract = []\n",
    "    if \"AGRI\" in ranges_list:\n",
    "        ranges_to_extract.append(AGRI_RANGE)\n",
    "    if \"FIN\" in ranges_list:\n",
    "        ranges_to_extract.append(FIN_RANGE)\n",
    "    if \"GEN\" in ranges_list:\n",
    "        ranges_to_extract.append(GENERAL_RANGE)\n",
    "\n",
    "    # Call the Sheets API\n",
    "    sheet = service.spreadsheets()\n",
    "    result = (\n",
    "        sheet.values()\n",
    "        .batchGet(spreadsheetId=SPREADSHEET_ID, ranges=ranges_to_extract)\n",
    "        .execute()\n",
    "    )\n",
    "    ranges = result.get(\"valueRanges\", [])\n",
    "\n",
    "    unscraped = []\n",
    "\n",
    "    if not os.path.exists(\"check\"):\n",
    "        os.mkdir(\"check\")\n",
    "    check_f = open(os.path.join(\"check\", \"check_content.txt\"), \"w\", encoding=\"utf-8\")\n",
    "\n",
    "    sheet_links = set()\n",
    "    link_count = 0\n",
    "    last_topic = \"\"\n",
    "\n",
    "    if not ranges:\n",
    "        print(\"No data found.\")\n",
    "    else:\n",
    "        print(f\"{len(ranges)} ranges retrieved\\n\")\n",
    "\n",
    "        # Iterate over all domains in ranges\n",
    "        for range_index, single_range in enumerate(ranges):\n",
    "            print(f\"In range {ranges_list[range_index]}\\n\")\n",
    "            range_count = 0\n",
    "\n",
    "            # Create folder if not already exists\n",
    "            folder_name = single_range[\"range\"].split(\"!\")[0]\n",
    "            if not os.path.exists(folder_name):\n",
    "                os.makedirs(folder_name)\n",
    "\n",
    "            for i, row in enumerate(single_range[\"values\"]):\n",
    "                index = 0\n",
    "                if len(row) != 0 and i != 0:\n",
    "\n",
    "                    # If topic exists in row\n",
    "                    if row[0]:\n",
    "                        last_topic = row[0]\n",
    "                        subtopic = \"\"\n",
    "                        if verbose:\n",
    "                            print(f\"Inside topic: {last_topic}\")\n",
    "\n",
    "                    # If subtopic exists in row\n",
    "                    if len(row) > 1 and row[1] != \"\":\n",
    "                        subtopic = row[1]\n",
    "                        if verbose:\n",
    "                            print(f\"\\tInside subtopic: {subtopic}\")\n",
    "\n",
    "                    for item in row:\n",
    "                        if validators.url(item):\n",
    "                            index += 1\n",
    "                            range_count += 1\n",
    "\n",
    "                            if (content := read_from_link(item)) == \"\":\n",
    "                                unscraped.append(item)\n",
    "                                continue\n",
    "                            else:\n",
    "                                check_f.write(item)\n",
    "                                check_f.write(\"\\n\")\n",
    "                                check_f.write(content)\n",
    "                                check_f.write(\"\\n\\n\")\n",
    "\n",
    "                            data = {\n",
    "                                \"topic\": last_topic,\n",
    "                                \"subtopic\": subtopic,\n",
    "                                \"url\": item,\n",
    "                                \"data\": content,\n",
    "                            }\n",
    "\n",
    "                            if subtopic:\n",
    "                                if not os.path.exists(\n",
    "                                    os.path.join(folder_name, last_topic, subtopic)\n",
    "                                ):\n",
    "                                    os.makedirs(\n",
    "                                        os.path.join(folder_name, last_topic, subtopic)\n",
    "                                    )\n",
    "                                with open(\n",
    "                                    os.path.join(\n",
    "                                        folder_name,\n",
    "                                        last_topic,\n",
    "                                        subtopic,\n",
    "                                        f\"{index}.json\",\n",
    "                                    ),\n",
    "                                    \"w\",\n",
    "                                    encoding=\"utf-8\",\n",
    "                                ) as f:\n",
    "                                    json.dump(data, f)\n",
    "                            else:\n",
    "                                if not os.path.exists(\n",
    "                                    os.path.join(folder_name, last_topic)\n",
    "                                ):\n",
    "                                    os.makedirs(os.path.join(folder_name, last_topic))\n",
    "                                with open(\n",
    "                                    os.path.join(\n",
    "                                        folder_name, last_topic, f\"{index}.json\"\n",
    "                                    ),\n",
    "                                    \"w\",\n",
    "                                    encoding=\"utf-8\",\n",
    "                                ) as f:\n",
    "                                    json.dump(data, f)\n",
    "\n",
    "            print(f\"\\nTotal links in {single_range['range']}:\\t{range_count}\\n\")\n",
    "            link_count += range_count\n",
    "\n",
    "        print(f\"\\nTotal links in sheet:\\t{link_count}\")\n",
    "\n",
    "    check_f.close()\n",
    "\n",
    "    with open(\"unscraped.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for link in unscraped:\n",
    "            f.write(link)\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5d17c5e4-5c44-45b2-959d-8ff30ca06791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 ranges retrieved\n",
      "\n",
      "In range AGRI\n",
      "\n",
      "\n",
      "Total links in AGRICULTURE!C1:S1236:\t357\n",
      "\n",
      "In range FIN\n",
      "\n",
      "\n",
      "Total links in FINANCE!C1:S1234:\t232\n",
      "\n",
      "In range GEN\n",
      "\n",
      "\n",
      "Total links in GENERAL!C1:S1943:\t820\n",
      "\n",
      "\n",
      "Total links in sheet:\t1409\n"
     ]
    }
   ],
   "source": [
    "extract([\"AGRI\", \"FIN\", \"GEN\"], verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0726a90f-c281-463f-843d-3beebd907e1b",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Verifying JSON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7aa7ba13-ec8a-4d74-bdeb-d45b6c400fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'Agricultural education',\n",
       " 'subtopic': '',\n",
       " 'url': 'https://hanka.gurturgoth.com/krishi-vigyan-kendra/',\n",
       " 'data': 'बेमेतरा 19 मार्च 2021। कृषि विज्ञान केन्द्र, कृषि महाविद्यालय अउ अनुसंधान केन्द्र ढोलिया बेमेतरा के सापर तत्वधान म बिरस्पत 18 मार्च के जिला स्तरीय किसान मेला सह संगोष्ठी के आयोजन करे गइस। मेला के मुख्य उद्देश्य अलसी व दलहनी फसल अउ खरीफ/रबी फसल मन के बीज उत्पादन ल प्रोत्साहित करना रिहिन। (मेला अखिल भारतीय समन्वित अलसी अनुसंधान परियोजना, अखिल भारतीय समन्वित मुलार्प अनुसंधान परियोजना अउ राष्ट्रीय बीज परियोजना- मेगा सीड परियोजना डाहर ले प्रायोजित रिहिन।) कार्यक्रम म मुख्य अतिथि कृषि मंत्री श्री रविन्द्र चौबे  विशिष्ट अतिथि विधायक बेमेतरा श्री आशीष छाबड़ा, डा. एस. के. पाटील (कुलपति इंदिरागांधी कृषि विश्वविद्यालय रायपुर) डा. एस. सी. मुखर्जी निदेशक विस्तार सेवायें, इंदिरागांधी कृषि विश्वविद्यालय रायपुर, डा. आर. के. द्विवेदी अधिष्ठाता कृषि महाविद्यालय अउ अनुसंधान केन्द्र, कवर्धा, डाॅ. डी. एस. ठाकुर अधिष्ठाता कृषि महाविद्यालय अउ अनुसंधान केन्द्र, साजा, बंशी पटेल, श्रीमति प्रज्ञा निर्वाणी (जिला पंचायत सदस्य बेमेतरा) के संग जिला, जनपद अउ पंचायत के आने प्रतिनिधि मन के गरिमामय उपस्थिति रहीन। जिला प्रशासन से श्री दुर्गेश वर्मा एस.डी.एम., उपसंचालक कृषि श्री एम. डी. मानकर, डाॅ. के पी वर्मा अधिष्ठाता कृषि माहाविद्यालय ढोलिया (बेमेतरा), एस.डी.ओ. सोलंकी शर्मा अउ जम्मो  ब्लाक के एस.ए. डी.ओ./आर. ए. इ.ओ. उप संचालक उपस्थित रिहिन।  कृषि मंत्री श्री रविन्द्र चौबे डाहर ले कृषि विज्ञान केन्द्र, कृषि महाविद्यालय अउ अनुसंधान केन्द्र, बेमेतरा के काम—काज अउ उदीम मन ल सहराए गहस। संगे —संग वैज्ञानिक मन ले कृषि क्षेत्र में किसान मन ल उन्नत कृषि कोति ले जाए अउ कृषि के भरोसा सशक्तिकरण के बात कहे गईन। विधायक के द्वारा भी कृषि और कृषकों को कृषि विज्ञान केन्द्र से मिलने वाले लाभों की सराहना की। किसान मेला म कृषि उद्यानिकी, मत्स्य व पशु विभाग के सहयोग रिहिन अउ स्टाल तको लगाये गेहे रिहिन। ये बेरा सोयाबीन सीड हब-बीज भण्डार गृह के भूमि पूजन अउ एनएचएम-एमएडीएच अंतर्गत स्थापित लघु मातृ वाटिका (नान्हे नर्सरी इकाई) के लोकार्पण तको करे गहस।'}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open(os.path.join(\"AGRICULTURE\", \"Agricultural education\", \"1.json\"))\n",
    "sample_data = json.load(f)\n",
    "sample_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24be3201-437e-414e-95ba-a27614dfea8a",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Search links from unadded, containing a particular substring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "687990c0-9052-467e-b856-6851f4bba5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3858 rows retrieved.\n"
     ]
    }
   ],
   "source": [
    "# Getting un-added links\n",
    "result = (\n",
    "    service.spreadsheets()\n",
    "    .values()\n",
    "    .get(spreadsheetId=SPREADSHEET_ID, range=UN_RANGE)\n",
    "    .execute()\n",
    ")\n",
    "unadded = result.get(\"values\", [])\n",
    "print(f\"{len(unadded)} rows retrieved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8c9670ae-509e-4b22-8e40-92944113ec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gg_links = pd.read_csv(os.path.join(\"unadded_links\", \"unadded_an_links.csv\"))\n",
    "an_links = pd.read_csv(os.path.join(\"unadded_links\", \"unadded_gg_links.csv\"))\n",
    "combined_links = pd.concat([gg_links, an_links])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "fc5abe17-8149-4fa0-9261-010a1f5bdc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_links(combined_links, substring_list):\n",
    "    \"\"\"\n",
    "    Gets all links from unadded that have a particular substring in them\n",
    "    \"\"\"\n",
    "    matching_links = []\n",
    "    for link in combined_links.iloc[:, 0].tolist():\n",
    "        for substring in substring_list:\n",
    "            if substring.lower() in link.lower():\n",
    "                matching_links.append(link)\n",
    "    return matching_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "83f669d4-7d5f-4cff-a119-1e3921ca3249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.anjor.online/2020/08/bhupesh-baghel-kisan-yojana.html',\n",
       " 'https://www.anjor.online/2020/05/kheti-kisani.html',\n",
       " 'https://www.anjor.online/2020/05/Rajiv-Gandhi-Kisan-Nyay-Yojana.html',\n",
       " 'https://www.anjor.online/2021/03/rajiv-gandhi-kisan-nayay-yojana.html',\n",
       " 'https://www.anjor.online/2020/04/kisan.html',\n",
       " 'https://www.anjor.online/2020/05/rajiv-gandhi-kisan-nyay-yojana.html',\n",
       " 'https://www.anjor.online/2021/05/bhupesh-baghel-cm-cg-kisan-naya-yoajan.html',\n",
       " 'https://hanka.gurturgoth.com/kisani-ke-goth/',\n",
       " 'https://hanka.gurturgoth.com/gujarat-kisan-sahayata/',\n",
       " 'https://hanka.gurturgoth.com/kisan-nalkup-connection/',\n",
       " 'https://hanka.gurturgoth.com/kisan-ke-pira/',\n",
       " 'https://hanka.gurturgoth.com/during-the-lockdown-the-bhoomgadi-mahila-kisan-group-served-food-access-to-the-house/',\n",
       " 'https://hanka.gurturgoth.com/kisan-sarkar-ke-pahili-prathmikata/',\n",
       " 'https://hanka.gurturgoth.com/kisan-man-ke-chehara/',\n",
       " 'https://hanka.gurturgoth.com/kisan-hit-nirnay/',\n",
       " 'https://hanka.gurturgoth.com/rajiv-gandhi-kisan-nyay-yojana-to-be-launched-on-21st-may-in-chhattisgarh/',\n",
       " 'https://hanka.gurturgoth.com/rajiv-gandhi-kisan-nyay-yojana/',\n",
       " 'https://hanka.gurturgoth.com/call-center-for-information-about-prime-minister-kisan-samman-nidhi/',\n",
       " 'https://hanka.gurturgoth.com/rajiv-gandhi-kisan-nyaya-yojana-kisan-dukhut-ram-received-one-lakh-rupees-in-first-installment/',\n",
       " 'https://hanka.gurturgoth.com/kondagaon-raghuram-kisan/',\n",
       " 'https://hanka.gurturgoth.com/sabal-kisan/',\n",
       " 'https://hanka.gurturgoth.com/krishi-vishwavidhyalaya/']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing\n",
    "find_links(combined_links, [\"kisan\", \"krishi\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df11b56e-b6ad-49b4-8eab-0f0a5ca546b6",
   "metadata": {},
   "source": [
    "## Get links that have all topics, subtopics from the sheet in them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "58510936-f217-464e-8dd5-34c70fd49d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 ranges retrieved.\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Call the Sheets API\n",
    "sheet = service.spreadsheets()\n",
    "result = sheet.values().batchGet(spreadsheetId=SPREADSHEET_ID, ranges=RANGES).execute()\n",
    "ranges = result.get(\"valueRanges\", [])\n",
    "\n",
    "sheet_links = set()\n",
    "link_count = 0\n",
    "last_topic = \"\"\n",
    "\n",
    "if not ranges:\n",
    "    print(\"No data found.\")\n",
    "else:\n",
    "    print(f\"{len(ranges)} ranges retrieved.\\n\")\n",
    "\n",
    "    # Iterate over all domains i.e Agri, Finance, General\n",
    "    for single_range in ranges:\n",
    "        range_list = set()\n",
    "\n",
    "        for i, row in enumerate(single_range[\"values\"]):\n",
    "            index = 0\n",
    "            if len(row) != 0 and i != 0:\n",
    "\n",
    "                # If topic exists in row\n",
    "                if row[0]:\n",
    "                    range_list.add(\"-\".join(row[0].strip(digits).lower().split()))\n",
    "                    # print(f\"Inside topic: {row[0]}\")\n",
    "\n",
    "                # If subtopic exists in row\n",
    "                if len(row) > 1 and row[1] != \"\":\n",
    "                    range_list.add(\"-\".join(row[1].strip(digits).lower().split()))\n",
    "                    # print(f\"\\tInside subtopic: {row[1]}\")\n",
    "\n",
    "        # Create file to store the lists\n",
    "        file_name = os.path.join(\n",
    "            \"unadded_links\",\n",
    "            \"unadded_\" + single_range[\"range\"].split(\"!\")[0].lower() + \".txt\",\n",
    "        )\n",
    "        with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "            for link in find_links(combined_links, list(range_list)):\n",
    "                f.write(link)\n",
    "                f.write(\"\\n\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48af9e09-ca32-4e4c-82c9-9a911810e69c",
   "metadata": {},
   "source": [
    "## Removing punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "fef8b3db-bb74-41c4-ae89-48a0b3714ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "\n",
    "    text = re.sub(r\"\\s*[a-zA-Z]\\s*\", \" \", text)\n",
    "\n",
    "    # Removing numbers\n",
    "    # text = re.sub('[0-9]', ' ' ,text)\n",
    "    # Replacing पूर्ण विराम with full stop\n",
    "    # text = re.sub('[।]','. ',text)\n",
    "\n",
    "    # text = re.sub(\"\\.\\s\", \"। \", text)\n",
    "\n",
    "    # Replacing all variants of single quatation marks with a uniform single quoration mark\n",
    "    text = re.sub(r\"\\s*’\\s*\", \"'\", text)\n",
    "    text = re.sub(r\"\\s*‘\\s*\", \"'\", text)\n",
    "    text = re.sub(r\"\\s*'\\s*\", \"'\", text)\n",
    "\n",
    "    # Replacing all variants of double quatation marks with a uniform double quoration mark\n",
    "    text = re.sub(r\"\\s*“\\s*\", '\"', text)\n",
    "    text = re.sub(r\"\\s*”\\s*\", '\"', text)\n",
    "    text = re.sub(r'\\s*\"\\s*', '\"', text)\n",
    "\n",
    "    # Unn-needed Character\n",
    "    text = re.sub(\"\\s*\\xa0\\s*\", \" \", text)\n",
    "\n",
    "    # Removing special symbols that are also used in regex so handling them separately\n",
    "    text = re.sub(r\"\\s*[+*&$^|\\\\]+\\s*\", \" \", text)\n",
    "\n",
    "    # Removing brackets\n",
    "    text = re.sub(r\"\\s*[\\([{})\\]]\\s*\", \" \", text)\n",
    "\n",
    "    # Removing other special symbols\n",
    "    text = re.sub(r\"\\s*,\\s*\", \" \", text)\n",
    "    text = re.sub(r\"\\s*;\\s*\", \" \", text)\n",
    "    text = re.sub(r\"\\s*:\\s*\", \" \", text)\n",
    "    text = re.sub(r\"\\s*—\\s*\", \" \", text)\n",
    "    text = re.sub(r\"\\s*-\\s*\", \" \", text)\n",
    "    text = re.sub(r\"\\s*_\\s*\", \" \", text)\n",
    "    text = re.sub(r\"\\s*@\\s*\", \" \", text)\n",
    "    text = re.sub(r\"\\s*#\\s*\", \" \", text)\n",
    "    text = re.sub(r\"\\s*%\\s*\", \" \", text)\n",
    "    text = re.sub(r\"\\s*=\\s*\", \" \", text)\n",
    "    text = re.sub(r\"\\s*/\\s*\", \" \", text)\n",
    "    text = re.sub(r\"\\s*<\\s*\", \" \", text)\n",
    "    text = re.sub(r\"\\s*>\\s*\", \" \", text)\n",
    "    text = re.sub(r\"\\s*●\\s*\", \" \", text)\n",
    "    text = re.sub(r\"\\s*~\\s*\", \" \", text)\n",
    "    text = re.sub(r\"\\s*`\\s*\", \" \", text)\n",
    "    text = re.sub(r\"\\s*–\\s*\", \" \", text)\n",
    "\n",
    "    # Replacing all variants of multiple whitespaces with single space\n",
    "    text = re.sub(r\"\\s\\s+\", \" \", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "27da0f21-8b38-4a4e-8cf3-7908895cade1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1859750\n",
      "350750\n",
      "22859\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['.',\n",
       " '.',\n",
       " '2',\n",
       " '19',\n",
       " 'मार्च',\n",
       " '2021।',\n",
       " 'हमर',\n",
       " 'प्रदेश',\n",
       " 'के',\n",
       " 'मुखिया',\n",
       " 'दाऊ',\n",
       " 'भूपेश',\n",
       " 'बघेल',\n",
       " 'ह',\n",
       " '21']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing\n",
    "check_content = open(\n",
    "    os.path.join(\"check\", \"check_content.txt\"), \"r\", encoding=\"utf-8\"\n",
    ").read()\n",
    "print(len(check_content))\n",
    "clean_content = remove_punct(check_content)\n",
    "words = clean_content.split()\n",
    "print(len(words))\n",
    "print(len(set(words)))\n",
    "words[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2b8b8f-eb96-46fb-9964-3b1430e3d475",
   "metadata": {},
   "source": [
    "## Creating word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "68f1d702-3861-4e81-b90a-d1d549d4e36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_count(domains):\n",
    "    \"\"\"\n",
    "    Creates word counts for each domain\n",
    "    \"\"\"\n",
    "    if all_doms := (domains == [\"AGRICULTURE\", \"FINANCE\", \"GENERAL\"]):\n",
    "        all_count = 0\n",
    "        all_dict = Counter()\n",
    "\n",
    "    for domain in domains:\n",
    "        word_count = 0\n",
    "        count_dict = Counter()\n",
    "        for root, dirs, files in os.walk(domain, topdown=True):\n",
    "            for file in files:\n",
    "                data = \"\"\n",
    "                if file.endswith(\".json\"):\n",
    "                    with open(os.path.join(root, file), \"r\", encoding=\"utf-8\") as f:\n",
    "                        data = json.load(f)[\"data\"]\n",
    "                elif file.endswith(\".txt\"):\n",
    "                    with open(os.path.join(root, file), \"r\", encoding=\"utf-8\") as f:\n",
    "                        data = f.read()\n",
    "                else:\n",
    "                    continue\n",
    "                cleaned_data = remove_punct(data)\n",
    "                words = cleaned_data.split()\n",
    "                for word in words:\n",
    "                    word = word.strip()\n",
    "                    bad_words = \"?!।‘’“”'\\\"\"\n",
    "                    word = word.translate(str.maketrans(\"\", \"\", bad_words))\n",
    "                    word = word.strip(digits).strip(\". \")\n",
    "                    if has_devnagri(word):\n",
    "                        count_dict[word] = count_dict.get(word, 0) + 1\n",
    "                        word_count += 1\n",
    "                        if all_doms:\n",
    "                            all_count += 1\n",
    "                            all_dict[word] = all_dict.get(word, 0) + 1\n",
    "                f.close()\n",
    "        if not os.path.exists(\"counts\"):\n",
    "            os.mkdir(\"counts\")\n",
    "\n",
    "        print(f\"\\nWord count for {domain} is {word_count}\")\n",
    "        print(f\"Unique words in {domain} are {len(count_dict.keys())}\")\n",
    "\n",
    "        file_name = os.path.join(\"counts\", domain.lower() + \"_count.csv\")\n",
    "        with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"word,count\\n\")\n",
    "            for k, v in count_dict.most_common():\n",
    "                f.write(f\"{k},{v}\\n\")\n",
    "\n",
    "    if all_doms:\n",
    "        print(f\"\\nWord count for all domains is {all_count}\")\n",
    "        print(f\"Unique words in all are {len(all_dict.keys())}\")\n",
    "\n",
    "        file_name = os.path.join(\"counts\", \"all_count.csv\")\n",
    "        with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"word,count\\n\")\n",
    "            for k, v in all_dict.most_common():\n",
    "                f.write(f\"{k},{v}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "de645054-bd85-4ca9-ab6a-882c66581551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word count for AGRICULTURE is 88909\n",
      "Unique words in AGRICULTURE are 8327\n",
      "\n",
      "Word count for FINANCE is 65425\n",
      "Unique words in FINANCE are 7140\n",
      "\n",
      "Word count for GENERAL is 281458\n",
      "Unique words in GENERAL are 22330\n",
      "\n",
      "Word count for all domains is 435792\n",
      "Unique words in all are 27218\n"
     ]
    }
   ],
   "source": [
    "create_word_count([\"AGRICULTURE\", \"FINANCE\", \"GENERAL\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625aa133-169b-40d6-ade4-70b5bc77b308",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de81d88-18fe-45a5-a0e9-afcd22a2bbb3",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Test: Check the number of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "3cb0a3e9-6db6-46d2-b30f-e97f4b6d5ecd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check the number of files in the domains\n",
    "def file_count(domains):\n",
    "    links = []\n",
    "    for domain in domains:\n",
    "        file_count = 0\n",
    "        for root, dirs, files in os.walk(domain):\n",
    "            for file in files:\n",
    "                if file.endswith(\".json\"):\n",
    "                    file_count += 1\n",
    "                    with open(os.path.join(root, file)) as f:\n",
    "                        links.append(json.load(f)[\"url\"])\n",
    "        print(f\"{domain}: {file_count}\")\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "6abc675a-7378-44eb-9fec-fdce62f10ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINANCE: 232\n",
      "AGRICULTURE: 357\n",
      "GENERAL: 812\n"
     ]
    }
   ],
   "source": [
    "fin_links = file_count([\"FINANCE\"])\n",
    "agri_links = file_count([\"AGRICULTURE\"])\n",
    "gen_links = file_count([\"GENERAL\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "ec227cbe-dd8f-46e3-9277-cb98a154f1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1091 rows retrieved.\n",
      "521 rows retrieved.\n",
      "415 rows retrieved.\n"
     ]
    }
   ],
   "source": [
    "# Getting general extra links\n",
    "result = (\n",
    "    service.spreadsheets()\n",
    "    .values()\n",
    "    .get(spreadsheetId=SPREADSHEET_ID, range=GENERAL_RANGE)\n",
    "    .execute()\n",
    ")\n",
    "gens = result.get(\"values\", [])\n",
    "print(f\"{len(gens)} rows retrieved.\")\n",
    "\n",
    "gen_links_sheet = []\n",
    "for row in gens:\n",
    "    for cell in row:\n",
    "        if validators.url(cell):\n",
    "            gen_links_sheet.append(cell)\n",
    "\n",
    "# Getting agriculture extra links\n",
    "result = (\n",
    "    service.spreadsheets()\n",
    "    .values()\n",
    "    .get(spreadsheetId=SPREADSHEET_ID, range=AGRI_RANGE)\n",
    "    .execute()\n",
    ")\n",
    "agris = result.get(\"values\", [])\n",
    "print(f\"{len(agris)} rows retrieved.\")\n",
    "\n",
    "agri_links_sheet = []\n",
    "for row in agris:\n",
    "    for cell in row:\n",
    "        if validators.url(cell):\n",
    "            agri_links_sheet.append(cell)\n",
    "\n",
    "# Getting finance extra links\n",
    "result = (\n",
    "    service.spreadsheets()\n",
    "    .values()\n",
    "    .get(spreadsheetId=SPREADSHEET_ID, range=FIN_RANGE)\n",
    "    .execute()\n",
    ")\n",
    "fins = result.get(\"values\", [])\n",
    "print(f\"{len(fins)} rows retrieved.\")\n",
    "\n",
    "fin_links_sheet = []\n",
    "for row in fins:\n",
    "    for cell in row:\n",
    "        if validators.url(cell):\n",
    "            fin_links_sheet.append(cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "2e2073f6-f6c8-4f12-8472-5465f7d57e48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "812 820\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['https://archive.org/details/GhandiMundiChhattisgarhiBalGeet/page/n7/mode/2up',\n",
       " 'http://chandrakala-soni.blogspot.com/',\n",
       " 'https://chhatrapalsahu1997.blogspot.com/2017/11/blog-post_38.html',\n",
       " 'https://omniglot.com/writing/chhattisgarhi.htm',\n",
       " 'http://kavitakosh.org/kk/%E0%A4%9B%E0%A4%A4%E0%A5%8D%E0%A4%A4%E0%A5%80%E0%A4%B8%E0%A4%97%E0%A4%A2%E0%A4%BC%E0%A5%80',\n",
       " 'https://chhattisgarhidictionary.learnchhattisgarhi.com/',\n",
       " 'https://chhattisgarhi4.rssing.com/chan-52874558/index-page1.html',\n",
       " 'https://hanka.gurturgoth.com/bijli-karmchari-adhikari-c0vid-protocal-palan-karai/',\n",
       " 'https://glosbe.com/en/hne',\n",
       " 'https://gurturgoth.com/ringani-raveli-saj-uday-shrivas/',\n",
       " 'https://www.scriptsource.org/cms/scripts/page.php?item_id=script_detail_sym&key=Deva',\n",
       " 'https://gurturgoth.com/category/%e0%a4%95%e0%a4%ac%e0%a4%bf%e0%a4%a4%e0%a4%be/',\n",
       " 'https://mpi-lingweb.shh.mpg.de/numeral/Chhattisgarhi.htm',\n",
       " 'https://dictionary.gurturgoth.com/',\n",
       " 'https://gurturgoth.com/milan-malariya-ke-kavita/',\n",
       " 'https://storyweaver.org.in/stories/222830-chhattisagadhi-alphabet']"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(gen_links), len(gen_links_sheet))\n",
    "list(set(gen_links_sheet) - set(gen_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "e088ed3b-4fa4-4e45-b7f0-bf0def0eb3e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357 357\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['https://www.chhattisgarh-gk.in/2020/06/names-of-fruits-vegetables-fauna-in.html']"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(agri_links), len(agri_links_sheet))\n",
    "list(set(agri_links_sheet) - set(agri_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d5427da9-c352-4dd0-a230-ca308035c0de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232 232\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(fin_links), len(fin_links_sheet))\n",
    "list(set(fin_links_sheet) - set(fin_links))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38aeaeab-d724-44f0-81f7-885cafd0f328",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65449df6-9ae6-46d8-b610-800478f0047e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Separating Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "71b96435-de7e-44c4-b0eb-58b1d6a86b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "test_string = \"बेमेतरा 19 मार्च 2021। कृषि विज्ञान केन्द्र, कृषि महाविद्यालय अउ अनुसंधान केन्द्र ढोलिया बेमेतरा के सापर तत्वधान म बिरस्पत 18 मार्च के जिला स्तरीय किसान मेला सह संगोष्ठी के आयोजन करे गइस। मेला के मुख्य उद्देश्य अलसी व दलहनी फसल अउ खरीफ/रबी फसल मन के बीज उत्पादन ल प्रोत्साहित करना रिहिन। (मेला अखिल भारतीय समन्वित अलसी अनुसंधान परियोजना, अखिल भारतीय समन्वित मुलार्प अनुसंधान परियोजना अउ राष्ट्रीय बीज परियोजना- मेगा सीड परियोजना डाहर ले प्रायोजित रिहिन।) कार्यक्रम म मुख्य अतिथि कृषि मंत्री श्री रविन्द्र चौबे  विशिष्ट अतिथि विधायक बेमेतरा श्री आशीष छाबड़ा, डा. एस. के. पाटील (कुलपति इंदिरागांधी कृषि विश्वविद्यालय रायपुर) डा. एस. सी. मुखर्जी निदेशक विस्तार सेवायें, इंदिरागांधी कृषि विश्वविद्यालय रायपुर, डा. आर. के. द्विवेदी अधिष्ठाता कृषि महाविद्यालय अउ अनुसंधान केन्द्र, कवर्धा, डाॅ. डी. एस. ठाकुर अधिष्ठाता कृषि महाविद्यालय अउ अनुसंधान केन्द्र, साजा, बंशी पटेल, श्रीमति प्रज्ञा निर्वाणी (जिला पंचायत सदस्य बेमेतरा) के संग जिला, जनपद अउ पंचायत के आने प्रतिनिधि मन के गरिमामय उपस्थिति रहीन। जिला प्रशासन से श्री दुर्गेश वर्मा एस.डी.एम., उपसंचालक कृषि श्री एम. डी. मानकर, डाॅ. के पी वर्मा अधिष्ठाता कृषि माहाविद्यालय ढोलिया (बेमेतरा), एस.डी.ओ. सोलंकी शर्मा अउ जम्मो  ब्लाक के एस.ए. डी.ओ./आर. ए. इ.ओ. उप संचालक उपस्थित रिहिन।  कृषि मंत्री श्री रविन्द्र चौबे डाहर ले कृषि विज्ञान केन्द्र, कृषि महाविद्यालय अउ अनुसंधान केन्द्र, बेमेतरा के काम—काज अउ उदीम मन ल सहराए गहस। संगे —संग वैज्ञानिक मन ले कृषि क्षेत्र में किसान मन ल उन्नत कृषि कोति ले जाए अउ कृषि के भरोसा सशक्तिकरण के बात कहे गईन। विधायक के द्वारा भी कृषि और कृषकों को कृषि विज्ञान केन्द्र से मिलने वाले लाभों की सराहना की। किसान मेला म कृषि उद्यानिकी, मत्स्य व पशु विभाग के सहयोग रिहिन अउ स्टाल तको लगाये गेहे रिहिन। ये बेरा सोयाबीन सीड हब-बीज भण्डार गृह के भूमि पूजन अउ एनएचएम-एमएडीएच अंतर्गत स्थापित लघु मातृ वाटिका (नान्हे नर्सरी इकाई) के लोकार्पण तको करे गहस।\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "ee3e692c-62fa-4b89-a648-6b6db3e188d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'बेमेतरा 19 मार्च 2021। कृषि विज्ञान केन्द्र कृषि महाविद्यालय अउ अनुसंधान केन्द्र ढोलिया बेमेतरा के सापर तत्वधान म बिरस्पत 18 मार्च के जिला स्तरीय किसान मेला सह संगोष्ठी के आयोजन करे गइस। मेला के मुख्य उद्देश्य अलसी व दलहनी फसल अउ खरीफ रबी फसल मन के बीज उत्पादन ल प्रोत्साहित करना रिहिन। मेला अखिल भारतीय समन्वित अलसी अनुसंधान परियोजना अखिल भारतीय समन्वित मुलार्प अनुसंधान परियोजना अउ राष्ट्रीय बीज परियोजना मेगा सीड परियोजना डाहर ले प्रायोजित रिहिन। कार्यक्रम म मुख्य अतिथि कृषि मंत्री श्री रविन्द्र चौबे विशिष्ट अतिथि विधायक बेमेतरा श्री आशीष छाबड़ा डा. एस. के. पाटील कुलपति इंदिरागांधी कृषि विश्वविद्यालय रायपुर डा. एस. सी. मुखर्जी निदेशक विस्तार सेवायें इंदिरागांधी कृषि विश्वविद्यालय रायपुर डा. आर. के. द्विवेदी अधिष्ठाता कृषि महाविद्यालय अउ अनुसंधान केन्द्र कवर्धा डाॅ. डी. एस. ठाकुर अधिष्ठाता कृषि महाविद्यालय अउ अनुसंधान केन्द्र साजा बंशी पटेल श्रीमति प्रज्ञा निर्वाणी जिला पंचायत सदस्य बेमेतरा के संग जिला जनपद अउ पंचायत के आने प्रतिनिधि मन के गरिमामय उपस्थिति रहीन। जिला प्रशासन से श्री दुर्गेश वर्मा एस.डी.एम. उपसंचालक कृषि श्री एम. डी. मानकर डाॅ. के पी वर्मा अधिष्ठाता कृषि माहाविद्यालय ढोलिया बेमेतरा एस.डी.ओ. सोलंकी शर्मा अउ जम्मो ब्लाक के एस.ए. डी.ओ. आर. ए. इ.ओ. उप संचालक उपस्थित रिहिन। कृषि मंत्री श्री रविन्द्र चौबे डाहर ले कृषि विज्ञान केन्द्र कृषि महाविद्यालय अउ अनुसंधान केन्द्र बेमेतरा के काम काज अउ उदीम मन ल सहराए गहस। संगे संग वैज्ञानिक मन ले कृषि क्षेत्र में किसान मन ल उन्नत कृषि कोति ले जाए अउ कृषि के भरोसा सशक्तिकरण के बात कहे गईन। विधायक के द्वारा भी कृषि और कृषकों को कृषि विज्ञान केन्द्र से मिलने वाले लाभों की सराहना की। किसान मेला म कृषि उद्यानिकी मत्स्य व पशु विभाग के सहयोग रिहिन अउ स्टाल तको लगाये गेहे रिहिन। ये बेरा सोयाबीन सीड हब बीज भण्डार गृह के भूमि पूजन अउ एनएचएम एमएडीएच अंतर्गत स्थापित लघु मातृ वाटिका नान्हे नर्सरी इकाई के लोकार्पण तको करे गहस।'"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "cleaned_test_string = remove_punct(test_string)\n",
    "cleaned_test_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "8d73688d-251d-4ed4-93ca-8e908ff654d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_delim(to_combine):\n",
    "    \"\"\"\n",
    "    To combine delimiting punctuation (!, ?, ।) marks with the sentences they are in\n",
    "    \"\"\"\n",
    "    combined_list = []\n",
    "    for i, sent in enumerate(to_combine):\n",
    "        if i and sent in [\"!\", \"?\", \"।\"]:\n",
    "            combined_list[-1] += sent\n",
    "        else:\n",
    "            combined_list.append(sent.strip())\n",
    "    return combined_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "8b89bd0a-25d4-4c28-90a8-733abf3f0184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentences(contents):\n",
    "    \"\"\"\n",
    "    Get a list of cleaned sentences\n",
    "    \"\"\"\n",
    "    cleaned_sentences = []\n",
    "    for sentence in combine_delim(list(filter(None, re.split(r\"([!?।])\", contents)))):\n",
    "        cleaned_sentence = remove_punct(sentence).strip(\". \")\n",
    "        if 8 < len(cleaned_sentence) < 1024 and has_devnagri(cleaned_sentence):\n",
    "            cleaned_sentences.append(cleaned_sentence)\n",
    "    return cleaned_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "02db3835-569a-4aee-9456-b0eb38988abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sentences(domains):\n",
    "    \"\"\"\n",
    "    Saves sentences for each domain\n",
    "    \"\"\"\n",
    "    if all_doms := (domains == [\"AGRICULTURE\", \"FINANCE\", \"GENERAL\"]):\n",
    "        all_sentences = []\n",
    "    for domain in domains:\n",
    "        dom_sentences = []\n",
    "        for root, dirs, files in os.walk(domain, topdown=True):\n",
    "            for file in files:\n",
    "                data = \"\"\n",
    "                if file.endswith(\".json\"):\n",
    "                    with open(os.path.join(root, file), \"r\", encoding=\"utf-8\") as f:\n",
    "                        data = json.load(f)[\"data\"]\n",
    "                elif file.endswith(\".txt\"):\n",
    "                    with open(os.path.join(root, file), \"r\", encoding=\"utf-8\") as f:\n",
    "                        data = f.read()\n",
    "                else:\n",
    "                    continue\n",
    "                sentences = clean_sentences(data)\n",
    "                dom_sentences.extend(sentences)\n",
    "\n",
    "        if all_doms:\n",
    "            all_sentences.extend(dom_sentences)\n",
    "\n",
    "        print(f\"Number of sentences in {domain}: {len(dom_sentences)}\")\n",
    "\n",
    "        if not os.path.exists(\"sentences\"):\n",
    "            os.mkdir(\"sentences\")\n",
    "\n",
    "        file_name = domain.lower() + \"_sentences.txt\"\n",
    "        with open(os.path.join(\"sentences\", file_name), \"w\", encoding=\"utf-8\") as f:\n",
    "            for sentence in dom_sentences:\n",
    "                f.write(f\"{sentence}\\n\")\n",
    "\n",
    "    if all_doms:\n",
    "        with open(\n",
    "            os.path.join(\"sentences\", \"all_sentences.txt\"), \"w\", encoding=\"utf-8\"\n",
    "        ) as f:\n",
    "            for sentence in all_sentences:\n",
    "                f.write(f\"{sentence}\\n\")\n",
    "        print(f\"Number of sentences in all: {len(all_sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "185a7dc4-b9e4-4ba0-8edd-96d3fd07cc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in AGRICULTURE: 3991\n",
      "Number of sentences in FINANCE: 2836\n",
      "Number of sentences in GENERAL: 19890\n",
      "Number of sentences in all: 26717\n"
     ]
    }
   ],
   "source": [
    "save_sentences([\"AGRICULTURE\", \"FINANCE\", \"GENERAL\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c51474e-92ce-4a11-8760-b72e63518ac3",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "de5bab45-d810-4ddf-bc56-57a7e57dea41",
   "metadata": {},
   "outputs": [],
   "source": [
    "chandrakala = open(\n",
    "    os.path.join(\"novels\", \"chandrakala.txt\"), \"r\", encoding=\"utf-8\"\n",
    ").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "24fc30f4-7332-4c87-b5a4-07cca7f870a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9244"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_sentences(chandrakala))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d3826677-2900-4100-9833-eb22092989c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mongra = open(os.path.join(\"novels\", \"mongra.txt\"), \"r\", encoding=\"utf-8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "025ac4a0-c931-496b-8d95-a585413839f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2082"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_sentences(mongra))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "83af0763-dcb3-42be-afad-3ad4073f41ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"अंजोर.नई दिल्ली। अस्पताल मन म ऑक्सीजन के उपलब्धता बढ़ाये के प्रधानमंत्री के निर्देश के तहत, पीएम केयर्स फंड ह देश म सार्वजनिक स्वास्थ्य केंद्र म 551 समर्पित पीएसए (प्रेशर स्विंग ऐड्सॉर्प्शन) चिकित्सा ऑक्सीजन उत्पादन संयंत्र के स्थापना के खातिर धन आवंटन के सैद्धांतिक मंजूरी दे दीस। पीएम ह निर्देश दे हावय के ये संयंत्र ल जल्दीध ले जल्दीी शुरू करे जाए। ओमन किहिन के ये संयंत्र ले जिला स्तर म ऑक्सीजन के उपलब्धता सुनिश्चित करे म काफी मदद मिलही।  ये समर्पित संयंत्र आने-आने राज्य/केंद्रशासित प्रदेश म जिला मुख्यालय म चिन्हित सरकारी अस्पताल मन म स्थापित करे जाही। खरीद प्रक्रिया स्वास्थ्य अउ परिवार कल्याण मंत्रालय के माध्यम ले करे जाही। पीएम केयर्स फंड ह ए साल के शुरुआत म देश म सार्वजनिक स्वास्थ्य केंद्र म अतिरिक्त 162 डेडिकेटेड प्रेशर स्विंग ऐड्सॉर्प्शन (पीएसए) मेडिकल ऑक्सीजन उत्पादन संयंत्र लगाये के खातिर 201.58 करोड़ रुपिया आवंटित करे रिहिस।  जिला मुख्यालय के सरकारी अस्पताल मन म पीएसए ऑक्सीजन उत्पादन संयंत्र स्थापित करे के मुख्य उद्देश्य सार्वजनिक स्वास्थ्य प्रणाली ल अउ मजबूत करना हावय अउ ये सुनिश्चित करना हावय के एमे ले सबोच अस्पताल मन म कैप्टिव ऑक्सीजन उत्पादन के सुविधा बने रइही। ए रकम ले अपन स्तर म ऑक्सीजन उत्पादन सुविधा ले ये अस्पताल मन अउ जिला के दिन-प्रतिदिन के मेडिकल ऑक्सीजन के जरूरत पूरी हो सकही। येकर अलावा, तरल चिकित्सा ऑक्सीजन (एलएमओ) कैप्टिव ऑक्सीजन उत्पादन के 'टॉप अप' के रूप म काम करही। ए रकम ले प्रणाली ये सुनिश्चित कर सकही के जिला के सरकारी अस्पताल मन के ऑक्सीजन के आपूर्ति म अचानक व्यवधान न उत्पन्न हो सके अउ कोरोना मरीज मन अउ आन जरूरतमंद मरीज मन के खातिर निर्बाध रूप ले पर्याप्त ऑक्सीजन मिल सके।\""
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_from_link(\"https://www.anjor.online/2021/04/nrendra-modi-pm-india.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870592c9-7055-4922-821f-cb02a167b263",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
